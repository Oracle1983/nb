{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 Advanced Topics in Data Science - Feature Preparation, Selection and Engineering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "In the last mission, we made our first submission to Kaggle, getting an accuracy score of 75.6%. While this is a good start, there is definitely room for improvement. There are two main areas we can focus on to boost the accuracy of our predictions:\n",
    "\n",
    "Improving the features we train our model on\n",
    "Improving the model itself\n",
    "In this mission, we're going to focus working with the features used in our model.\n",
    "\n",
    "We'll start by looking at feature selection. Feature selection is important because it helps to exclude features which are not good predictors, or features that are closely related to each other. Both of these will cause our model to be less accurate, particularly on previously unseen data.\n",
    "\n",
    "The diagram below illustrates this. The red dots represent the data we are trying to predict, and each of the blue lines represents a different model.\n",
    "\n",
    "Overfitting\n",
    "https://s3.amazonaws.com/dq-content/186/overfitting.svg\n",
    "\n",
    "The model on the left is overfitting, which means the model represents the training data too closely, and is unlikely to predict well on unseen data, like the holdout data for our Kaggle competition.\n",
    "\n",
    "The model on the right is well-fit. It captures the underlying pattern in the data without the detailed noise found just in the training set. A well fit model is likely to make accurate predictions on previously unseen data. The key to creating a well-fit model is to select the right balance of features, and to create new features to train your model.\n",
    "\n",
    "In the previous mission, we trained our model using data about the age, sex and class of the passengers on the Titanic. Let's start by using the functions we created in that mission to add the columns we had at the end of the first mission.\n",
    "\n",
    "Remember that any modifications we make to our training data (train.csv) we also have to make to our holdout data (test.csv).\n",
    "\n",
    "Instructions\n",
    "Use the process_age() function:\n",
    "To convert the Age column in train, assigning the result to train.\n",
    "To convert the Age column in holdout, assigning the result to holdout.\n",
    "Create a for loop which iterates over the column names \"Age_categories\", \"Pclass\", and \"Sex\". In each iteration:\n",
    "Use the create_dummies() function to process the train dataframe for the given column, assigning the result to train.\n",
    "Use the create_dummies() function to process the holdout dataframe for the given column, assigning the result to holdout.\n",
    "Use the print() function to display the columns in train using train.columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "holdout = pd.read_csv('test.csv')\n",
    "\n",
    "def process_age(df):\n",
    "    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n",
    "    cut_points = [-1,0,5,12,18,35,60,100]\n",
    "    label_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\n",
    "    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n",
    "    return df\n",
    "\n",
    "def create_dummies(df,column_name):\n",
    "    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n",
    "    df = pd.concat([df,dummies],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Preparing More Features\n",
    "\n",
    "Our model in the previous mission was based on three columns from the original data: Age, Sex, and Pclass. As you saw when you printed the column names in the previous screen, there are a number of other columns that we haven't yet used. To make it easier to reference, the output from the previous screen is copied below:\n",
    "\n",
    "Index(['PassengerId', 'Survived', 'Pclass', 'Name',\n",
    "       'Sex', 'Age', 'SibSp', 'Parch', 'Ticket',\n",
    "       'Fare', 'Cabin', 'Embarked', 'Age_categories',\n",
    "       'Age_categories_Missing',\n",
    "       'Age_categories_Infant',\n",
    "       'Age_categories_Child',\n",
    "       'Age_categories_Teenager',\n",
    "       'Age_categories_Young Adult',\n",
    "       'Age_categories_Adult',\n",
    "       'Age_categories_Senior', 'Pclass_1',\n",
    "       'Pclass_2', 'Pclass_3','Sex_female',\n",
    "       'Sex_male'], dtype='object')\n",
    "The last nine rows of the output are dummy columns we created, but in the first three rows we can see there are a number of features we haven't yet utilized. We can ignore PassengerId, since this is just a column Kaggle have added to identify each passenger and calculate scores. We can also ignore Survived, as this is what we're predicting, as well as the three columns we've already used.\n",
    "\n",
    "Here is a list of the remaining columns (with a brief description), followed by 10 randomly selected passengers from and their data from those columns, so we can refamiliarize ourselves with the data.\n",
    "\n",
    "SibSp - The number of siblings or spouses the passenger had aboard the Titanic\n",
    "Parch - The number of parents or children the passenger had aboard the Titanic\n",
    "Ticket - The passenger's ticket number\n",
    "Fare - The fair the passenger paid\n",
    "Cabin - The passengers cabin number\n",
    "Embarked - The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)\n",
    "\n",
    "At first glance, both the Name and Ticket columns look to be unique to each passenger. We will come back to these columns later, but for now we'll focus on the other columns.\n",
    "\n",
    "We can use the Dataframe.describe() [http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html] method to give us some more information on the values within each remaining column.\n",
    "\n",
    ">>> columns = ['SibSp','Parch','Fare','Cabin','Embarked']\n",
    ">>> train[columns].describe(include='all',percentiles=[])\n",
    "                 SibSp       Parch        Fare Cabin Embarked\n",
    "    count   891.000000  891.000000  891.000000   204      889\n",
    "    unique         NaN         NaN         NaN   147        3\n",
    "    top            NaN         NaN         NaN    G6        S\n",
    "    freq           NaN         NaN         NaN     4      644\n",
    "    mean      0.523008    0.381594   32.204208   NaN      NaN\n",
    "    std       1.102743    0.806057   49.693429   NaN      NaN\n",
    "    min       0.000000    0.000000    0.000000   NaN      NaN\n",
    "    50%       0.000000    0.000000   14.454200   NaN      NaN\n",
    "    max       8.000000    6.000000  512.329200   NaN      NaN\n",
    "Of these, SibSp, Parch and Fare look to be standard numeric columns with no missing values. Cabin has values for only 204 of the 891 rows, and even then most of the values are unique, so for now we will leave this column also. Embarked looks to be a standard categorical column with 3 unique values, much like PClass was, except that there are two missing values. We can easily fill these two missing values with the most common value, \"S\" which occurs 644 times.\n",
    "\n",
    "Looking at our numeric columns, we can see a big difference between the range of each. SibSp has values between 0-8, Parch between 0-6, and Fare is on a dramatically different scale, with values ranging from 0-512. In order to make sure these values are equally weighted within our model, we'll need to rescale the data.\n",
    "\n",
    "Rescaling simply stretches or shrinks the data as needed to be on the same scale, in our case between 0 and 1.\n",
    "\n",
    "An example of rescaling\n",
    "https://s3.amazonaws.com/dq-content/186/rescaling.svg\n",
    "\n",
    "In the diagram above, the three columns have different minimum and maximum values before rescaling.\n",
    "\n",
    "After rescaling, the values in each feature has been compressed or stretched so that they are all on the same scale - they have the same minimum and maximum, and the relationship between each point is still the same relative other points in that feature. You can now easily see that the data represented in each column is identical.\n",
    "\n",
    "Within scikit-learn, the preprocessing.minmax_scale() [http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html] function allows us to quickly and easily rescale our data:\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "columns = [\"column one\", \"column two\"]\n",
    "data[columns] = min_max_scale(data[columns])\n",
    "Let's process the Embarked, SibSp, Parch and Fare columns in both our train and holdout dataframes.\n",
    "\n",
    "Instructions\n",
    "For both the train and holdout dataframes:\n",
    "Use the Series.fillna() [https://www.dataquest.io/m/186/feature-preparation%2C-selection-and-engineering/2/pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.fillna.html] method to replace any missing values in the Embarked column with \"S\"\n",
    "Use our create_dummies() function to create dummy columns for the Embarked column.\n",
    "Use minmax_scale() to rescale the SibSp, Parch, and Fare columns, assigning the results back to new columns SibSp_scaled, Parch_scaled. and Fare_scaled respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "# The holdout set has a missing value in the Fare column which\n",
    "# we'll fill with the mean.\n",
    "holdout[\"Fare\"] = holdout[\"Fare\"].fillna(train[\"Fare\"].mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Determining the Most Relevant Features\n",
    "\n",
    "In order to select the best-performing features, we need a way to measure which of our features are relevant to our outcome - in this case, the survival of each passenger. One effective way is by training a logistic regression model using all of our features, and then looking at the coefficients of each feature.\n",
    "\n",
    "The scikit-learn LogisticRegression class has an attribute in which coefficients are stored after the model is fit, LogisticRegression.coef_. We first need to train our model, after which we can access this attribute.\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_X,train_y)\n",
    "coefficients = lr.coef_\n",
    "The coef() method returns a NumPy array of coefficients, in the same order as the features that were used to fit the model. To make these easier to interpret, we can convert the coefficients to a pandas series, adding the column names as the index:\n",
    "\n",
    "feature_importance = pd.Series(coefficients[0],\n",
    "                               index=train_X.columns)\n",
    "We'll now fit a model and plot the coefficients for each feature.\n",
    "\n",
    "Instructions\n",
    "Instantiate a LogisticRegression() object.\n",
    "Fit the LogisticRegression object using the columns from the list columns from the train dataframe and the target column Survived.\n",
    "Use the coef_ attribute to retrieve the coefficients of the features, and assign the results to coefficients.\n",
    "Create a series object using coefficients, with the feature column names as the index and assign it to feature_importance.\n",
    "Use the Series.plot.barh() [http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.plot.barh.html] method to plot the feature_importance series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "columns = ['Age_categories_Missing', 'Age_categories_Infant',\n",
    "       'Age_categories_Child', 'Age_categories_Teenager',\n",
    "       'Age_categories_Young Adult', 'Age_categories_Adult',\n",
    "       'Age_categories_Senior', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n",
    "       'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n",
    "       'SibSp_scaled', 'Parch_scaled', 'Fare_scaled']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Training a model using relevant features.\n",
    "\n",
    "The plot we generated in the last screen showed a range of both positive and negative values. Whether the value is positive or negative isn't as important in this case, relative to the magnitude of the value. If you think about it, this makes sense. A feature that indicates strongly whether a passenger died is just as useful as a feature that indicates strongly that a passenger survived, given they are mutually exclusive outcomes.\n",
    "\n",
    "To make things easier to interpret, we'll alter the plot to show all positive values, and have sorted the bars in order of size:\n",
    "\n",
    "ordered_feature_importance = feature_importance.abs().sort_values()\n",
    "ordered_feature_importance.plot.barh()\n",
    "plt.show()\n",
    "\n",
    "Feature Importance\n",
    "https://s3.amazonaws.com/dq-content/186/feature_importance.png\n",
    "\n",
    "We'll train a new model with the top 8 scores and check our accuracy using cross validation.\n",
    "\n",
    "Instructions\n",
    "Instantiate a LogisticRegression() object.\n",
    "Use the model_selection.cross_val_score() [http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score] function and assign the returned object to scores, using:\n",
    "The columns specified in columns and all rows from the train dataframe.\n",
    "A cv parameter of 10.\n",
    "Calculate the mean of the cross validation scores and assign the results to accuracy.\n",
    "Use the print() function to display the variable accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "columns = ['Age_categories_Infant', 'SibSp_scaled', 'Sex_female', 'Sex_male',\n",
    "       'Pclass_1', 'Pclass_3', 'Age_categories_Senior', 'Parch_scaled']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. Submitting our Improved Model to Kaggle\n",
    "\n",
    "The cross validation score of 81.48% is marginally higher than the cross validation score for the model we created in the previous mission, which had a score of 80.2%.\n",
    "\n",
    "Hopefully, this improvement will translate to previously unseen data. Let's train a model using the columns from the previous step, make some predictions on the holdout data and submit it to Kaggle for scoring.\n",
    "\n",
    "Instructions\n",
    "Instantiate a LogisticRegression() object and fit it using all_X and all_y.\n",
    "Use the predict() method to make predictions using the same columns in the holdout dataframe, and assign the result to holdout_predictions\n",
    "Create a dataframe submission with two columns:\n",
    "PassengerId, with the values from the PassengerId column of the holdout dataframe.\n",
    "Survived, with the values from holdout_predictions.\n",
    "Use the DataFrame.to_csv method to save the submission dataframe to the filename submission_1.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['Age_categories_Infant', 'SibSp_scaled', 'Sex_female', 'Sex_male',\n",
    "       'Pclass_1', 'Pclass_3', 'Age_categories_Senior', 'Parch_scaled']\n",
    "\n",
    "all_X = train[columns]\n",
    "all_y = train['Survived']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. Engineering a New Feature Using Binning\n",
    "\n",
    "You can download the CSV from the previous step here [https://s3.amazonaws.com/dq-content/186/submission_1.csv]. When you submit it to Kaggle, you'll see that the store is 77.0%, which at the time of writing equates to jumping about 1,500 places up the leaderboard (this will vary as the leaderboard is always changing). It's only a small improvement, but we're moving in the right direction.\n",
    "\n",
    "A lot of the gains in accuracy in machine learning come from Feature Engineering. Feature engineering is the practice of creating new features from your existing data.\n",
    "\n",
    "One common way to engineer a feature is using a technique called binning. Binning is when you take a continuous feature, like the fare a passenger paid for their ticket, and separate it out into several ranges (or 'bins'), turning it into a categorical variable.\n",
    "\n",
    "This can be useful when there are patterns in the data that are non-linear and you're using a linear model (like logistic regression). We actually used binning in the previous mission when we dealt with the Age column, although we didn't use the term.\n",
    "\n",
    "Let's look at histograms of the Fare column for passengers who died and survived, and see if there are patterns that we can use when creating our bins.\n",
    "\n",
    "Histogram of the Fare column, survived vs died\n",
    "https://s3.amazonaws.com/dq-content/186/fare_histogram.png\n",
    "\n",
    "Looking at the values, it looks like we can separate the feature into four bins to capture some patterns from the data:\n",
    "\n",
    "0-12\n",
    "12-50\n",
    "50-100\n",
    "100+\n",
    "Like in the previous mission, we can use the pandas.cut() [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html] function to create our bins.\n",
    "\n",
    "Instructions\n",
    "Using the process_age() function as a model, create a function process_fare() that uses the pandas cut() method to create bins for the Fare column and assign the results to a new column called Fare_categories.\n",
    "We have already dealt with missing values in the Fare column, so you won't need the line that uses fillna().\n",
    "Use the process_fare() function on both the train and holdout dataframes, creating the four 'bins':\n",
    "0-12, for values between 0 and 12.\n",
    "12-50, for values between 12 and 50.\n",
    "50-100, for values between 50 and 100.\n",
    "100+, for values between 100 and 1000.\n",
    "Use the create_dummies() function on both the train and holdout dataframes to create dummy columns for the Fare column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_age(df,cut_points,label_names):\n",
    "    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n",
    "    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7. Engineering Features From Text Columns\n",
    "\n",
    "Another way to engineer features is by extracting data from text columns. Earlier, we decided that the Name and Cabin columns weren't useful by themselves, but what if there is some data there we could extract? Let's take a look at a random sample of rows from those two columns:\n",
    "\n",
    "Name\tCabin\n",
    "772\tMack, Mrs. (Mary)\tE77\n",
    "148\tNavratil, Mr. Michel (\"Louis M Hoffman\")\tF2\n",
    "707\tCalderhead, Mr. Edward Pennington\tE24\n",
    "879\tPotter, Mrs. Thomas Jr (Lily Alexenia Wilson)\tC50\n",
    "21\tBeesley, Mr. Lawrence\tD56\n",
    "456\tMillet, Mr. Francis Davis\tE38\n",
    "97\tGreenfield, Mr. William Bertram\tD10 D12\n",
    "263\tHarrison, Mr. William\tB94\n",
    "393\tNewell, Miss. Marjorie\tD36\n",
    "759\tRothes, the Countess. of (Lucy Noel Martha Dye...\tB77\n",
    "While in isolation the cabin number of each passenger will be reasonably unique to each, we can see that the format of the cabin numbers is one letter followed by two numbers. It seems like the letter is representative of the type of cabin, which could be useful data for us. We can use the pandas Series.str accessor and then subset the first character using brackets:\n",
    "\n",
    ">>> train.head()[\"Cabin\"]\n",
    "​\n",
    "    0     NaN\n",
    "    1     C85\n",
    "    2     NaN\n",
    "    3    C123\n",
    "    4     NaN\n",
    "    Name: Cabin, dtype: object\n",
    "​\n",
    ">>> train.head()[\"Cabin\"].str[0]\n",
    "​\n",
    "    0    NaN\n",
    "    1      C\n",
    "    2    NaN\n",
    "    3      C\n",
    "    4    NaN\n",
    "    Name: Cabin, dtype: object\n",
    "Looking at the Name column, There is a title like 'Mr' or 'Mrs' within each, as well as some less common titles, like the 'Countess' from the final row of our table above. By spending some time researching the different titles, we can categorize these into six types:\n",
    "\n",
    "Mr\n",
    "Mrs\n",
    "Master\n",
    "Miss\n",
    "Officer\n",
    "Royalty\n",
    "We can use the Series.str.extract method and a regular expression to extract the title from each name and then use the Series.map() method and a predefined dictionary to simplify the titles.\n",
    "\n",
    "titles = {\n",
    "    \"Mme\":         \"Mrs\",\n",
    "    \"Ms\":          \"Mrs\",\n",
    "    \"Mrs\" :        \"Mrs\",\n",
    "    \"Countess\":    \"Royalty\",\n",
    "    \"Lady\" :       \"Royalty\"\n",
    "}\n",
    "​\n",
    "extracted_titles = train[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "train[\"Title\"] = extracted_titles.map(titles)\n",
    "\n",
    "Instructions\n",
    "Use extract(), map() and the dictionary titles to categorize the titles for the holdout dataframe and assign the results to a new column Title.\n",
    "For both the train and holdout dataframes:\n",
    "Use the str() accessor to extract the first letter from the Cabin column and assign the result to a new column Cabin_type.\n",
    "Use the fillna() method to fill any missing values in Cabin_type with \"Unknown\"\n",
    "For the newly created columns Title and Cabin_type, use create_dummies() to create dummy columns for both the train and holdout dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = {\n",
    "    \"Mr\" :         \"Mr\",\n",
    "    \"Mme\":         \"Mrs\",\n",
    "    \"Ms\":          \"Mrs\",\n",
    "    \"Mrs\" :        \"Mrs\",\n",
    "    \"Master\" :     \"Master\",\n",
    "    \"Mlle\":        \"Miss\",\n",
    "    \"Miss\" :       \"Miss\",\n",
    "    \"Capt\":        \"Officer\",\n",
    "    \"Col\":         \"Officer\",\n",
    "    \"Major\":       \"Officer\",\n",
    "    \"Dr\":          \"Officer\",\n",
    "    \"Rev\":         \"Officer\",\n",
    "    \"Jonkheer\":    \"Royalty\",\n",
    "    \"Don\":         \"Royalty\",\n",
    "    \"Sir\" :        \"Royalty\",\n",
    "    \"Countess\":    \"Royalty\",\n",
    "    \"Dona\":        \"Royalty\",\n",
    "    \"Lady\" :       \"Royalty\"\n",
    "}\n",
    "\n",
    "extracted_titles = train[\"Name\"].str.extract(' ([A-Za-z]+)\\.',expand=False)\n",
    "train[\"Title\"] = extracted_titles.map(titles)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8. Finding Correlated Features\n",
    "\n",
    "We now have 34 possible feature columns we can use to train our model. One thing to be aware of as you start to add more features is a concept called collinearity. Collinearity occurs where more than one feature contains data that are similar.\n",
    "\n",
    "The effect of collinearity is that your model will overfit - you may get great results on your test data set, but then the model performs worse on unseen data (like the holdout set).\n",
    "\n",
    "One easy way to understand collinearity is with a simple binary variable like the Sex column in our dataset. Every passenger in our data is categorized as either male or female, so 'not male' is exactly the same as 'female'.\n",
    "\n",
    "As a result, when we created our two dummy columns from the categorical Sex column, we've actually created two columns with identical data in them. This will happen whenever we create dummy columns, and is called the dummy variable trap [http://www.algosome.com/articles/dummy-variable-trap-regression.html]. The easy solution is to choose one column to drop any time you make dummy columns.\n",
    "\n",
    "Collinearity can happen in other places, too. A common way to spot collinearity is to plot correlations between each pair of variables in a heatmap. An example of this style of plot is below:\n",
    "\n",
    "Correlation Heatmap\n",
    "https://s3.amazonaws.com/dq-content/186/corr_heatmap.png\n",
    "\n",
    "The darker squares, whether the darker red or darker blue, indicate pairs of columns that have higher correlation and may lead to collinearity. The easiest way to produce this plot is using the DataFrame.corr() [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html] method to produce a correlation matrix, and then use the Seaborn library's seaborn.heatmap() [https://seaborn.pydata.org/generated/seaborn.heatmap.html] function to plot the values:\n",
    "\n",
    "import seaborn as sns\n",
    "correlations = train.corr()\n",
    "sns.heatmap(correlations)\n",
    "plt.show()\n",
    "The example plot above was produced using a code example [http://seaborn.pydata.org/examples/many_pairwise_correlations.html] from seaborn's documentation which produces an correlation heatmap that is easier to interpret than the default output of heatmap(). We've created a function containing that code to make it easier for you to plot the correlations between the features in our data.\n",
    "\n",
    "Instructions\n",
    "Use the plot_correlation_heatmap() function to produce a heatmap for the train dataframe, using only the features in the list columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_correlation_heatmap(df):\n",
    "    corr = df.corr()\n",
    "    \n",
    "    sns.set(style=\"white\")\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.show()\n",
    "\n",
    "columns = ['Age_categories_Missing', 'Age_categories_Infant',\n",
    "       'Age_categories_Child', 'Age_categories_Teenager',\n",
    "       'Age_categories_Young Adult', 'Age_categories_Adult',\n",
    "       'Age_categories_Senior', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n",
    "       'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n",
    "       'SibSp_scaled', 'Parch_scaled', 'Fare_categories_0-12',\n",
    "       'Fare_categories_12-50','Fare_categories_50-100', 'Fare_categories_100+',\n",
    "       'Title_Master', 'Title_Miss', 'Title_Mr','Title_Mrs', 'Title_Officer',\n",
    "       'Title_Royalty', 'Cabin_type_A','Cabin_type_B', 'Cabin_type_C', 'Cabin_type_D',\n",
    "       'Cabin_type_E','Cabin_type_F', 'Cabin_type_G', 'Cabin_type_T', 'Cabin_type_Unknown']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "9. Final Feature Selection using RFECV\n",
    "\n",
    "The plot we created in the previous screen is reproduced below:\n",
    "\n",
    "Train correlation heatmap\n",
    "https://s3.amazonaws.com/dq-content/186/train_heatmap.png\n",
    "\n",
    "We can see that there is a high correlation between Sex_female/Sex_male and Title_Miss/Title_Mr/Title_Mrs. We will remove the columns Sex_female and Sex_male since the title data may be more nuanced.\n",
    "\n",
    "Apart from that, we should remove one of each of our dummy variables to reduce the collinearity in each. We'll remove:\n",
    "\n",
    "Pclass_2\n",
    "Age_categories_Teenager\n",
    "Fare_categories_12-50\n",
    "Title_Master\n",
    "Cabin_type_A\n",
    "\n",
    "In an earlier step, we manually used the logit coefficients to select the most relevant features. An alternate method is to use one of scikit-learn's inbuilt feature selection classes. We will be using the feature_selection.RFECV [http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html] class which performs recursive feature elimination with cross-validation.\n",
    "\n",
    "The RFECV class starts by training a model using all of your features and scores it using cross validation. It then uses the logit coefficients to eliminate the least important feature, and trains and scores a new model. At the end, the class looks at all the scores, and selects the set of features which scored highest.\n",
    "\n",
    "Like the LogisticRegression class, RFECV must first be instantiated and then fit. The first parameter when creating the RFECV object must be an estimator, and we need to use the cv parameter to specific the number of folds for cross-validation.\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "lr = LogisticRegression()\n",
    "selector = RFECV(lr,cv=10)\n",
    "selector.fit(all_X,all_y)\n",
    "Once the RFECV object has been fit, we can use the RFECV.support_ attribute to access a boolean mask of True and False values which we can use to generate a list of optimized columns:\n",
    "\n",
    "optimized_columns = all_X.columns[selector.support_]\n",
    "\n",
    "Instructions\n",
    "Instantiate a LogisticRegression() object, lr.\n",
    "Instantiate a RFECV() object selector using the newly created lr object and cv=10 as parameters.\n",
    "Use the fit() method to fit selector using all_X and all_y\n",
    "Use the support_ attribute selector to subset all_X.columns, and assign the result to optimized_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "columns = ['Age_categories_Missing', 'Age_categories_Infant',\n",
    "       'Age_categories_Child', 'Age_categories_Young Adult',\n",
    "       'Age_categories_Adult', 'Age_categories_Senior', 'Pclass_1', 'Pclass_3',\n",
    "       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'SibSp_scaled',\n",
    "       'Parch_scaled', 'Fare_categories_0-12', 'Fare_categories_50-100',\n",
    "       'Fare_categories_100+', 'Title_Miss', 'Title_Mr', 'Title_Mrs',\n",
    "       'Title_Officer', 'Title_Royalty', 'Cabin_type_B', 'Cabin_type_C',\n",
    "       'Cabin_type_D', 'Cabin_type_E', 'Cabin_type_F', 'Cabin_type_G',\n",
    "       'Cabin_type_T', 'Cabin_type_Unknown']\n",
    "\n",
    "all_X = train[columns]\n",
    "all_y = train[\"Survived\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "10. Training A Model Using our Optimized Columns\n",
    "\n",
    "The RFECV() selector returned only four columns:\n",
    "\n",
    "['SibSp_scaled', 'Title_Mr', 'Title_Officer', 'Cabin_type_Unknown']\n",
    "Let's train a model using cross validation using these columns and check the score.\n",
    "\n",
    "Instructions\n",
    "Instantiate LogisticRegression() object.\n",
    "Use the model_selection.cross_val_score() [http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score] function and assign the results to scores, using:\n",
    "all_X and all_y.\n",
    "A cv parameter of 10.\n",
    "Calculate the mean of the cross validation scores and assign the results to accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_X = train[optimized_columns]\n",
    "all_y = train[\"Survived\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11. Submitting our Model to Kaggle\n",
    "\n",
    "This four-feature model scores 82.3%, a modest improvement compared to the 81.5% from our earlier model. Let's train these columns on the holdout set, save a submission file and see what score we get from Kaggle.\n",
    "\n",
    "Instructions\n",
    "Instantiate a LogisticRegression() object and fit it using all_X and all_y.\n",
    "Use the predict() method to make predictions using the same columns in the holdout dataframe, and assign the result to holdout_predictions.\n",
    "Create a dataframe submission with two columns:\n",
    "PassengerId, with the values from the PassengerId column of the holdout dataframe.\n",
    "Survived, with the values from holdout_predictions.\n",
    "Use the DataFrame.to_csv method to save the submission dataframe to the filename submission_2.csv."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "12. Next Steps\n",
    "\n",
    "You can download the submission file we just created here [https://s3.amazonaws.com/dq-content/186/submission_2.csv] and submit it to Kaggle. The score this submission gets is 78.0%, which is equivalent to a jump of roughly 1,000 spots (again, this will vary as submission are constantly being made to the leaderboard).\n",
    "\n",
    "Second submission to Kaggle\n",
    "By preparing, engineering and selecting features, we have increased our accuracy by 2.4%. When working in Kaggle competitions, you should spend a lot of time experimenting with features, particularly feature engineering.\n",
    "\n",
    "Here are some ideas that you can use to work with features for this competition:\n",
    "\n",
    "Use SibSp and Parch to explore total relatives onboard.\n",
    "Create combinations of multiple columns, for instance Pclass + Sex.\n",
    "See if you can extract useful data out of the Ticket column.\n",
    "Try different combinations of features to see if you can identify features that overfit less than others.\n",
    "In the next mission in this course, we'll look at selecting and optimizing different models to improve our score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
