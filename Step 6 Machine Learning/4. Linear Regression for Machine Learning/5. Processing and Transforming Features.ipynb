{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Machine Learning -  Processing and Transforming Features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "To understand how linear regression works, we've stuck to using features from the training dataset that contained no missing values and were already in a convenient numeric representation. In this mission, we'll explore how to transform some of the the remaining features so we can use them in our model. Broadly, the process of processing and creating new features is known as feature engineering. Feature engineering [https://en.wikipedia.org/wiki/Feature_engineering] is a bit of an art and having knowledge in the specific domain (in this case real estate) can help you create better features. In this mission, we'll focus on some domain-independent strategies that work for all problems.\n",
    "\n",
    "In the first half of this mission, we'll focus only on columns that contain no missing values but still aren't in the proper format to use in a linear regression model. In the latter half of this mission, we'll explore some ways to deal with missing values.\n",
    "\n",
    "Amongst the columns that don't contain missing values, some of the common issues include:\n",
    "\n",
    "the column is not numerical (e.g. a zoning code represented using text)\n",
    "the column is numerical but not ordinal (e.g. zip code values)\n",
    "the column is numerical but isn't representative of the type of relationship with the target column (e.g. year values)\n",
    "Let's start by filtering the training set to just the columns containing no missing values.\n",
    "\n",
    "Instructions\n",
    "Select just the columns from the train data frame that contain no missing values.\n",
    "Assign the resulting data frame, that contains just these columns, to df_no_mv.\n",
    "Use the variables display to become familiar with these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "train_null_counts = train.isnull().sum()\n",
    "print(train_null_counts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Categorical Features\n",
    "\n",
    "You'll notice that some of the columns in the data frame df_no_mv contain string values. If these columns contain only a limited set of uniuqe values, they're known as categorical features. As the name suggests, a categorical feature groups a specific training example into a specific category. Here are some examples from the dataset:\n",
    "\n",
    ">>> train['Utilities'].value_counts()\n",
    "AllPub    1457\n",
    "NoSewr       2\n",
    "NoSeWa       1\n",
    "Name: Utilities, dtype: int64\n",
    "​\n",
    ">>> train['Street'].value_counts()\n",
    "Pave    1455\n",
    "Grvl       5\n",
    "​\n",
    ">>> train['House Style'].value_counts()\n",
    "1Story    743\n",
    "2Story    440\n",
    "1.5Fin    160\n",
    "SLvl       60\n",
    "SFoyer     35\n",
    "2.5Unf     11\n",
    "1.5Unf      8\n",
    "2.5Fin      3\n",
    "\n",
    "To use these features in our model, we need to transform them into numerical representations. Thankfully, pandas makes this easy because the library has a special categorical data type[https://pandas.pydata.org/pandas-docs/stable/categorical.html]. We can convert any column that contains no missing values (or an error will be thrown) to the categorical data type using the pandas.Series.astype() method:\n",
    "\n",
    ">>> train['Utilities'] = train['Utilities'].astype('category')\n",
    "When a column is converted to the categorical data type, pandas assigns a code to each unique value in the column. Unless we access these values directly, most of the pandas manipulation operations that work for string columns will work for categorical ones as well.\n",
    "\n",
    ">>> train['Utilities']\n",
    "0       AllPub\n",
    "1       AllPub\n",
    "2       AllPub\n",
    "3       AllPub\n",
    "4       AllPub\n",
    "5       AllPub\n",
    "...\n",
    "We need to use the .cat accessor followed by the .codes property to actually access the underlying numerical representation of a column:\n",
    "\n",
    ">>> train['Utilities'].cat.codes\n",
    "Let's convert all of the text columns that contain no missing values into the categorical data type.\n",
    "\n",
    "Instructions\n",
    "Convert all of the text columns in train to the categorical data type.\n",
    "Select the Utilities column, return the categorical codes, and display the unique value counts for those codes: train['Utilities'].cat.codes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_cols = df_no_mv.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in text_cols:\n",
    "    print(col+\":\", len(train[col].unique()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Dummy Coding\n",
    "\n",
    "When we convert a column to the categorical data type, pandas assigns a number from 0 to n-1 (where n is the number of unique values in a column) for each value. The drawback with this approach is that one of the assumptions of linear regression is violated here. Linear regression operates under the assumption that the features are linearly correlated with the target column. For a categorical feature, however, there's no actual numerical meaning to the categorical codes that pandas assigned for that colum. An increase in the Utilities column from 1 to 2 has no correlation value with the target column, and the categorical codes are instead used for uniqueness and exclusivity (the category associated with 0 is different than the one associated with 1).\n",
    "\n",
    "The common solution is to use a technique called dummy coding [https://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29]. Instead of having a single column with n integer codes, we have n binary columns. Here's what that would look like for the Utilities column:\n",
    "\n",
    "Utilities_AllPub\n",
    "Utilities_NoSewr\tUtilities_NoSeWa\n",
    "1\t0\t0\n",
    "1\t0\t0\n",
    "1\t0\t0\n",
    "1\t0\t0\n",
    "\n",
    "Because the original values for the first 4 rows were AllPub, in the new scheme, they contain the binary value for true (1) in the Utilities_AllPub column and 0 for the other 2 columns.\n",
    "\n",
    "Pandas thankfully has a convenience method to help us apply this transformation for all of the text columns called pandas.get_dummies():\n",
    "\n",
    "dummy_cols = pd.get_dummies()\n",
    " \n",
    "Instructions\n",
    "Convert all of the columns in text_cols from the train data frame into dummy columns.\n",
    "Delete the original columns from text_cols from the train data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_cols = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Transforming Improper Numerical Features\n",
    "\n",
    "In the last few screens, we focused on categorical values that were represented as text columns. Some of the numerical columns in the data set are also categorical and only have a limited set of unique values. We won't explicitly explore those coumns in this mission, but the feature transformation process is the same if the numbers used in those categories have no numerical meaning.\n",
    "\n",
    "Let's now look at numerical features that aren't categorical, but whose numerical representation needs to be improved. We'll focus on the Year Remod/Add and Year Built columns:\n",
    "\n",
    ">>> train[['Year Remod/Add', 'Year Built']]\n",
    "0   1960    1960\n",
    "1   1961    1961\n",
    "2   1958    1958\n",
    "3   1968    1968\n",
    "4   1998    1997\n",
    "...\n",
    "The two main issues with these features are:\n",
    "\n",
    "Year values aren't representative of how old a house is\n",
    "The Year Remod/Add column doesn't actually provide useful information for a linear regression model\n",
    "The challenge with year values like 1960 and 1961 is that they don't do a good capture how old a house is. For example, a house that was built in 1960 but sold in 1980 was sold in half the time one built in 1960 and sold in 2000. Instead of the years certain events happened, we want the difference between those years. We should create a new column that's the difference between both of these columns.\n",
    "\n",
    "For this particular piece of information (years until remodeled), this is a sensible approach. Domain knowledge can help you understand how to best transform features to represent information well for a linear model. If you're ever confused about a feature or how it should be represented, reading scientific papers or posts by researchers in the specific domain is critical. Many winners of Kaggle data science competitions, for example, claim that their focus on data preparation and feature engineering combined with common machine learning models helped them win.\n",
    "\n",
    "Instructions\n",
    "Create a new column years_until_remod in the train data frame that represents the difference between Year Remod/Add (the later value) and Year Built (the earlier value)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. Missing Values\n",
    "\n",
    "In the next few screens, we'll focus on handling columns with missing values. When values are missing in a column, there are two main approaches we can take:\n",
    "\n",
    "Remove rows containing missing values for specific columns\n",
    "Pro: Rows containing missing values are removed, leaving only clean data for modeling\n",
    "Con: Entire observations from the training set are removed, which can reduce overall prediction accuracy\n",
    "Impute (or replace) missing values using a descriptive statistic from the column\n",
    "Pro: Missing values are replaced with potentially similar estimates, preserving the rest of the observation in the model.\n",
    "Con: Depending on the approach, we may be adding noisy data for the model to learn\n",
    "Given that we only have 1460 training examples (with ~80 potentially useful features), we don't want to remove any of these rows from the dataset. Let's instead focus on imputation techniques.\n",
    "\n",
    "We'll focus on columns that contain at least 1 missing value but less than 365 missing values (or 25% of the number of rows in the training set). There's no strict threshold, and many people instead use a 50% cutoff (if half the values in a column are missing, it's automatically dropped). Having some domain knowledge can help with determining an acceptable cutoff value.\n",
    " \n",
    "Instructions\n",
    "Select only the columns from train that contain more than 0 missing values but less than 584 missing values. Assign the resulting data frame to df_missing_values.\n",
    "Display the number of missing values for each column in df_missing_values.\n",
    "Display the data type for each column in df_missing_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "train_null_counts = train.isnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. Imputing Missing Values\n",
    "\n",
    "It looks like about half of the columns in df_missing_values are string columns (object data type), while about half are float64 columns. For numerical columns with missing values, a common strategy is to compute the mean, median, or mode of each column and replace all missing values in that column with that value.\n",
    "\n",
    "Because imputation is a common task, pandas contains a pandas.DataFrame.fillna() [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html] method that we can use for this. If we pass in a value, all of the missing values (NaN) in the data frame are replaced by that value:\n",
    "\n",
    "# Only select float columns.\n",
    "missing_floats = df_missing_vals.select_dtypes(include=['float'])\n",
    "​\n",
    "# Returns a data frame with missing values replaced with 0.\n",
    "fill_with_zero = missing_floats.fillna(0)\n",
    "You can also pass in a column-wise summarization function and fill in missing values that way:\n",
    "\n",
    "# Returns a data frame with missing values replaced with mean of that column.\n",
    "fill_with_mean = missing_floats.fillna(missing_floats.mean())\n",
    "Let's impute all of the missing values in float columns with each column's mean.\n",
    "\n",
    "Instructions\n",
    "Impute the missing values from float_cols with the column's mean.\n",
    "Check for any missing values in float_cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "float_cols = df_missing_values.select_dtypes(include=['float'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
