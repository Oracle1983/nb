{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Machine Learning -  Project Walkthrough Data Cleaning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Recap\n",
    "\n",
    "In the past mission, you removed all of the columns that contained redundant information, weren't useful for modeling, required too much processing to make useful, or leaked information from the future. We've exported the Dataframe from the end of the last mission to a CSV file named filtered_loans_2007.csv to differentiate the file with the loans_2007.csv we used in the last mission. In this mission, we'll prepare the data for machine learning by focusing on handling missing values, converting categorical columns to numeric columns, and removing any other extraneous columns we encounter throughout this process.\n",
    "\n",
    "This is because the mathematics underlying most machine learning models assumes that the data is numerical and contains no missing values. To reinforce this requirement, scikit-learn will return an error if you try to train a model using data that contain missing values or non-numeric values when working with models like linear regression and logistic regression.\n",
    "\n",
    "Let's start by computing the number of missing values and come up with a strategy for handling them. Then, we'll focus on the categorical columns.\n",
    "\n",
    "We can return the number of missing values across the Dataframe by:\n",
    "\n",
    "first using the Pandas Dataframe method isnull [http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isnull.html] to return a Dataframe containing Boolean values:\n",
    "True if the original value is null,\n",
    "False if the original value isn't null.\n",
    "then using the Pandas Dataframe method sum to calculate the number of null values in each column.\n",
    "\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "Instructions\n",
    "Read in filtered_loans_2007.csv as a Dataframe and assign it to loans.\n",
    "Use the isnull and sum methods to return the number of null values in each column. Assign the resulting Series object to null_counts.\n",
    "Use the print function to display null_counts."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Handling missing values\n",
    "\n",
    "While most of the columns have 0 missing values, 2 columns have 50 or less rows with missing values, and 1 column, pub_rec_bankruptcies, contains 697 rows with missing values. Let's remove columns entirely where more than 1% of the rows for that column contain a null value. In addition, we'll remove the remaining rows containing null values.\n",
    "\n",
    "This means that we'll keep the following columns and just remove rows containing missing values for them:\n",
    "\n",
    "title\n",
    "revol_util\n",
    "last_credit_pull_d\n",
    "and drop the pub_rec_bankruptcies column entirely since more than 1% of the rows have a missing value for this column.\n",
    "\n",
    "Let's use the strategy of removing the pub_rec_bankruptcies column first then removing all rows containing any missing values at all to cover both of these cases. This way, we only remove the rows containing missing values for the title and revol_util columns but not the pub_rec_bankruptcies column.\n",
    "\n",
    "Instructions\n",
    "Use the drop [http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html] method to remove the pub_rec_bankruptcies column from loans.\n",
    "Use the dropna method [http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html] to remove all rows from loans containing any missing values.\n",
    "Use the dtypes attribute followed by the value_counts() method to return the counts for each column data type. Use the print function to display these counts."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Text columns\n",
    "\n",
    "While the numerical columns can be used natively with scikit-learn, the object columns that contain text need to be converted to numerical data types. Let's return a new Dataframe containing just the object columns so we can explore them in more depth. You can use the Dataframe method select_dtypes [http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html] to select only the columns of a certain data type:\n",
    "\n",
    "float_df = df.select_dtypes(include=['float'])\n",
    "Let's select just the object columns then display a sample row to get a better sense of how the values in each column are formatted.\n",
    "\n",
    "Instructions\n",
    "Use the Dataframe method select_dtypes to select only the columns of object type from loans and assign the resulting Dataframe object_columns_df.\n",
    "Display the first row in object_columns_df using the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Converting text columns\n",
    "\n",
    "Some of the columns seem like they represent categorical values, but we should confirm by checking the number of unique values in those columns:\n",
    "\n",
    "home_ownership: home ownership status, can only be 1 of 4 categorical values according to the data dictionary,\n",
    "verification_status: indicates if income was verified by Lending Club,\n",
    "emp_length: number of years the borrower was employed upon time of application,\n",
    "term: number of payments on the loan, either 36 or 60,\n",
    "addr_state: borrower's state of residence,\n",
    "purpose: a category provided by the borrower for the loan request,\n",
    "title: loan title provided the borrower,\n",
    "There are also some columns that represent numeric values, that need to be converted:\n",
    "\n",
    "int_rate: interest rate of the loan in %,\n",
    "revol_util: revolving line utilization rate or the amount of credit the borrower is using relative to all available credit, read more here.\n",
    "Based on the first row's values for purpose and title, it seems like these columns could reflect the same information. Let's explore the unique value counts separately to confirm if this is true.\n",
    "\n",
    "Lastly, some of the columns contain date values that would require a good amount of feature engineering for them to be potentially useful:\n",
    "\n",
    "earliest_cr_line: The month the borrower's earliest reported credit line was opened,\n",
    "last_credit_pull_d: The most recent month Lending Club pulled credit for this loan.\n",
    "Since these date features require some feature engineering for modeling purposes, let's remove these date columns from the Dataframe."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. First 5 categorical columns\n",
    "\n",
    "Let's explore the unique value counts of the columnns that seem like they contain categorical values.\n",
    "\n",
    "Instructions\n",
    "Display the unique value counts for the following columns: home_ownership, verification_status, emp_length, term, addr_state columns:\n",
    "Store these column names in a list named cols.\n",
    "Use a for loop to iterate over cols:\n",
    "Use the print function combined with the Series method value_counts to display each column's unique value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['home_ownership', 'verification_status', 'emp_length', 'term', 'addr_state']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. The reason for the loan\n",
    "\n",
    "The home_ownership, verification_status, emp_length, term, and addr_state columns all contain multiple discrete values. We should clean the emp_length column and treat it as a numerical one since the values have ordering (2 years of employment is less than 8 years).\n",
    "\n",
    "First, let's look at the unique value counts for the purpose and title columns to understand which column we want to keep.\n",
    "\n",
    "Instructions\n",
    "Use the value_counts method and the print function to display the unique values in the following columns:\n",
    "purpose\n",
    "title"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "7. Categorical columns\n",
    "\n",
    "The home_ownership, verification_status, emp_length, and term columns each contain a few discrete categorical values. We should encode these columns as dummy variables and keep them.\n",
    "\n",
    "It seems like the purpose and title columns do contain overlapping information but we'll keep the purpose column since it contains a few discrete values. In addition, the title column has data quality issues since many of the values are repeated with slight modifications (e.g. Debt Consolidation and Debt Consolidation Loan and debt consolidation).\n",
    "\n",
    "We can use the following mapping to clean the emp_length column:\n",
    "\n",
    "\"10+ years\": 10\n",
    "\"9 years\": 9\n",
    "\"8 years\": 8\n",
    "\"7 years\": 7\n",
    "\"6 years\": 6\n",
    "\"5 years\": 5\n",
    "\"4 years\": 4\n",
    "\"3 years\": 3\n",
    "\"2 years\": 2\n",
    "\"1 year\": 1\n",
    "\"< 1 year\": 0\n",
    "\"n/a\": 0\n",
    "We erred on the side of being conservative with the 10+ years, < 1 year and n/a mappings. We assume that people who may have been working more than 10 years have only really worked for 10 years. We also assume that people who've worked less than a year or if the information is not available that they've worked for 0. This is a general heuristic but it's not perfect.\n",
    "\n",
    "Lastly, the addr_state column contains many discrete values and we'd need to add 49 dummy variable columns to use it for classification. This would make our Dataframe much larger and could slow down how quickly the code runs. Let's remove this column from consideration.\n",
    "\n",
    "Instructions\n",
    "Remove the last_credit_pull_d, addr_state, title, and earliest_cr_line columns from loans.\n",
    "Convert the int_rate and revol_util columns to float columns by:\n",
    "Using the str acessor followed by the rstrip string method to strip the right trailing percent sign (%):\n",
    "loans['int_rate'].str.rstrip('%') returns a new Series with % stripped from the right side of each value.\n",
    "On the resulting Series object, use the astype method to convert to the float type.\n",
    "Assign the new Series of float values back to the respective columns in the Dataframe.\n",
    "Use the replace method to clean the emp_length column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping_dict = {\n",
    "    \"emp_length\": {\n",
    "        \"10+ years\": 10,\n",
    "        \"9 years\": 9,\n",
    "        \"8 years\": 8,\n",
    "        \"7 years\": 7,\n",
    "        \"6 years\": 6,\n",
    "        \"5 years\": 5,\n",
    "        \"4 years\": 4,\n",
    "        \"3 years\": 3,\n",
    "        \"2 years\": 2,\n",
    "        \"1 year\": 1,\n",
    "        \"< 1 year\": 0,\n",
    "        \"n/a\": 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8. Dummy variables\n",
    "\n",
    "Let's now encode the home_ownership, verification_status, title, and term columns as dummy variables so we can use them in our model. We first need to use the Pandas get_dummies [http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html] method to return a new Dataframe containing a new column for each dummy variable:\n",
    "\n",
    "# Returns a new Dataframe containing 1 column for each dummy variable.\n",
    "dummy_df = pd.get_dummies(loans[\"term\", \"verification_status\"])\n",
    "We can then use the concat [http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html] method to add these dummy columns back to the original Dataframe:\n",
    "\n",
    "loans = pd.concat([loans, dummy_df], axis=1]\n",
    "and then drop the original columns entirely using the drop method:\n",
    "\n",
    "loans = loans.drop([\"verification_status\", \"term\"], axis=1)\n",
    "\n",
    "Instructions\n",
    "Encode the home_ownership, verification_status, purpose, and term columns as integer values:\n",
    "Use the Series method astype to convert each column to the category data type.\n",
    "Use the get_dummies function to return a Dataframe containing the dummy columns.\n",
    "Use the concat method to add these dummy columns back to loans.\n",
    "Remove the original, non-dummy columns (home_ownership, verification_status, purpose, and term) from loans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
