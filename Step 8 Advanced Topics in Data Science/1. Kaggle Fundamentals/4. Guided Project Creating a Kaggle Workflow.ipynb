{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 Advanced Topics in Data Science -  Guided Project Creating a Kaggle Workflow"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Introducing Data Science Workflows\n",
    "\n",
    "So far in this course, you've been learning about Kaggle competitions using Dataquest missions. Missions are highly structured and your work is answer checked every step of the way.\n",
    "\n",
    "Guided projects, on the other hand, are less structured and focus more on exploration. Guided projects help you synthesize concepts learned during missions and practice what you have learned.\n",
    "\n",
    "Guided projects bridge the gap between learning using the Dataquest missions, and applying the knowledge on your own computer, and your answers are not checked like they are in regular missions, however you can access a solution notebook by using the top of the interface.\n",
    "\n",
    "Working with Guided projects is a great opportunity to practice some of the extra skills you'll need to do data science by yourself, including practicing debugging using all the tools at your disposal, including googling for answers, visiting Stack Overflow and consulting the documentation for the modules you are using.\n",
    "\n",
    "This guided project uses Jupyter notebook, a web application which lets you combine text and code within a single file, and is one of the most popular ways to explore and iterate when working with data. The Jupyter notebook easily allows you to share your work, and makes exploring data much easier.\n",
    "\n",
    "If you're not familiar with Jupyter notebook, we recommend completing our guided project on Using Jupyter notebook to familiarize yourself.\n",
    "\n",
    "In this guided project, we're going to put together all that we've learned in this course and create a data science workflow.\n",
    "\n",
    "Data science, and particularly machine learning, contain many dimensions of complexity when compared with standard software development. In standard software development, code not working as you expect can be caused by a number of factors along two dimensions:\n",
    "\n",
    "Bugs in implementation\n",
    "Algorithm design\n",
    "Machine learning problems, have many more dimensions:\n",
    "\n",
    "Bugs in implementation\n",
    "Algorithm design\n",
    "Model issues\n",
    "Data quality\n",
    "The result of this is that there are exponentially more places that machine learning can go wrong.\n",
    "\n",
    "Why is machine learning 'hard'?\n",
    "https://s3.amazonaws.com/dq-content/188/why-is-ml-hard.png\n",
    "\n",
    "This concept is shown in the diagram above (taken from the excellent post Why is machine learning 'hard'?) [http://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html]. The green dot is a 'correct' solution, where the red dots are incorrect solutions. In this illustration there are only a small number of incorrect combinations for software engineering, but in machine learning this becomes exponentially greater!\n",
    "\n",
    "By defining a workflow for yourself, you can give yourself a framework with which to make iterating on ideas quicker and easier, allowing yourself to work more efficiently.\n",
    "\n",
    "In this mission, we're going to explore a workflow to make competing in the Kaggle Titanic competition easier, using a pipeline of functions to reduce the number of dimensions you need to focus on.\n",
    "\n",
    "To get started, we'll read in the original train.csv and test.csv files from Kaggle.\n",
    "\n",
    "Instructions\n",
    "Import the pandas library.\n",
    "Use pandas to import the file train.csv as train.\n",
    "Use pandas to import the file test.csv as holdout.\n",
    "Display the first few lines of the test dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Preprocessing the Data\n",
    "\n",
    "One of the many benefits of using Jupyter is that (by default) it uses the IPython kernel to run code. This gives you all the benefits of IPython, including code completion and 'magic' commands. (If you'd like to read more about the internals of Jupyter and how it can help you work more efficiently, you might like to check out our blog post Jupyter Notebook Tips, Tricks and Shortcuts.)\n",
    "\n",
    "We can use one of those magic commands, the [http://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-load] %load command, to load an external file. The %load command will copy the contents of the file into the current notebook cell. The syntax is simple:\n",
    "\n",
    "%load [filename]\n",
    "To illustrate, say we had a file called test.py with the following line of code:\n",
    "\n",
    "print(\"This is test.py\")\n",
    "To use load, we simply type the following into a Jupyter cell:\n",
    "\n",
    "%load test.py\n",
    "Once we run that cell, the contents from the test.py file would be copied into the cell:\n",
    "\n",
    "# %load test.py\n",
    "print(\"This is test.py\")\n",
    "If we ran the cell one more time, the code would run, giving us the output This is test.py.\n",
    "\n",
    "We have created a file, functions.py which contains versions of the functions we created in the earlier missions form this course, which will save you building those functions again from scratch.\n",
    "\n",
    "Let's import that file and preprocess our Kaggle data.\n",
    "\n",
    "Instructions\n",
    "Use the %load magic command to load the contents of functions.py into a notebook cell and read through the functions you have imported.\n",
    "Create a new function, which:\n",
    "Accepts a dataframe parameter\n",
    "Applies the process_missing(), process_age(), process_fare(), process_titles(), and process_cabin() functions to the dataframe\n",
    "Applies the create_dummies() function to the \"Age_categories\", \"Fare_categories\",\"Title\", \"Cabin_type\", and \"Sex\" columns.\n",
    "Returns the processed dataframe\n",
    "Apply the newly create function on the train and holdout dataframes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Exploring the Data\n",
    "\n",
    "In the first three missions of this course, we have done a variety of activities, mostly in isolation: Exploring the data, creating features, selecting features, selecting and tuning different models.\n",
    "\n",
    "The Kaggle workflow we are going to build will combine all of these into a process.\n",
    "\n",
    "Kaggle Workflow\n",
    "https://s3.amazonaws.com/dq-content/188/kaggle_workflow.svg\n",
    "\n",
    "Data exploration, to find patterns in the data\n",
    "Feature engineering, to create new features from those patterns or through pure experimentation\n",
    "Feature selection, to select the best subset of our current set of features\n",
    "Model selection/tuning, training a number of models with different hyperparameters to find the best performer.\n",
    "We can continue to repeat this cycle as we work to optimize our predictions. At the end of any cycle we wish, we can also use our model to make predictions on the holdout set and then Submit to Kaggle to get a leaderboard score.\n",
    "\n",
    "While the first two steps of our workflow are relatively freeform, later in this project we'll create some functions that will help automate the complexity of the latter two steps so we can move faster.\n",
    "\n",
    "For now, let's practice the first stage, exploring the data. We're going to examine the two columns that contain information about the family members each passenger had onboard: SibSp and Parch.\n",
    "\n",
    "If you need some help with techniques for exploring and visualizing data, you might like to check out our Data Analysis with Pandas and Exploratory Data Visualization courses.\n",
    "\n",
    "Instructions\n",
    "Review the data dictionary and variable notes for the Titanic competition on Kaggle's website to familiarize yourself with the SibSp and Parch columns.\n",
    "Use pandas and matplotlib to explore those two columns. You might like to try:\n",
    "Inspecting the type of the columns\n",
    "Using histograms to view the distribution of values in the columns\n",
    "Use pivot tables to look at the survival rate for different values of the columns\n",
    "Find a way to combine the columns and look at the resulting distribution of values and survival rate\n",
    "Write a markdown cell explaining your findings."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Engineering New Features\n",
    "\n",
    "You should have discovered in the previous step that by combining the values of SibSp and Parch into a single column, only 30% of the passengers who had no family members onboard survived.\n",
    "\n",
    "If you didn't get this conclusion, you can use the code segment below to verify this for yourself:\n",
    "\n",
    "explore_cols = [\"SibSp\",\"Parch\",\"Survived\"]\n",
    "explore = train[explore_cols].copy()\n",
    "â€‹\n",
    "explore['familysize'] = explore[[\"SibSp\",\"Parch\"]].sum(axis=1)\n",
    "pivot = explore.pivot_table(index=col,values=\"Survived\")\n",
    "pivot.plot.bar(ylim=(0,1),yticks=np.arange(0,1,.1))\n",
    "plt.show()\n",
    "Based of this, we can come up with an idea for a new feature - was the passenger alone. This will be a binary column containing the value:\n",
    "\n",
    "1 if the passenger has zero family members onboard\n",
    "0 if the passenger has one or more family members onboard\n",
    "Let's go ahead and create this feature.\n",
    "\n",
    "Instructions\n",
    "Create a function, that:\n",
    "Accepts a dataframe as input\n",
    "Adds a new column, isalone that has the value 0 if the passenger has one or more family members onboard, and 1 if the passenger has zero family members onboard.\n",
    "Returns the new dataframe\n",
    "Apply the newly created function to the train and holdout dataframes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. Selecting the Best-Performing Features\n",
    "\n",
    "The next step in our workflow is feature selection. In the Feature Preparation, Selection and Engineering mission, we used scikit-learn's feature_selection.RFECV [https://www.dataquest.io/m/188/guided-project%3A-creating-a-kaggle-workflow/5/scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html] class to automate selecting the best-performing features using recursive feature elimination.\n",
    "\n",
    "To speed up our Kaggle workflow, we can create a function that performs this step for us, which will mean we can perform feature selection by calling a self-contained function and focus our efforts on the more creative part - exploring the data and engineering new features.\n",
    "\n",
    "You may remember that the first parameter when you instantiate a RFECV() object is an estimator. At the time we used a Logistic Regression estimator, but we've since discovered in the Model Selection and Tuning mission that Random Forests seems to be a better algorithm for this Kaggle competition.\n",
    "\n",
    "Let's write a function that:\n",
    "\n",
    "Accepts a dataframe as input\n",
    "Performs data preparation for machine learning\n",
    "Uses recursive feature elimination and the random forests algorithm to find the best-performing set of features \n",
    "\n",
    "Instructions\n",
    "Import feature_selection.RFECV and ensemble.RandomForestClassifier\n",
    "Create a function, select_features(), that:\n",
    "Accepts a dataframe as input\n",
    "Removes any non-numeric columns or columns containing null values\n",
    "Creates all_X and all_y variables, making sure that all_X contains neither the PassengerId or Survived columns.\n",
    "Uses feature_selection.RFECV and ensemble.RandomForestClassifier to perform recursive feature elimination using:\n",
    "all_X and all_y\n",
    "A random state of 1\n",
    "10 fold cross validation\n",
    "Prints a list of the best columns from recursive feature elimination\n",
    "Returns a list of the best columns from recursive feature elimination\n",
    "Run the newly created function using the train dataframe as input and assign the result to a variable."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. Selecting and Tuning Different Algorithms\n",
    "\n",
    "Just like we did with feature selection, we can can write a function to do the heavy lifting of model selection and tuning. The function we'll create will use three different algorithms and use grid search to train using different combinations of hyperparameters to find the best performing model.\n",
    "\n",
    "We can achieve this by creating a list of dictionariesâ€” that is, a list where each element of the list is a dictionary. Each dictionary should contain:\n",
    "\n",
    "The name of the particular model\n",
    "An estimator object for the model\n",
    "A dictionary of hyperparameters that we'll use for grid search.\n",
    "Here's an example of what one of these dictionaries will look like:\n",
    "\n",
    "{\n",
    "    \"name\": \"KNeighborsClassifier\",\n",
    "    \"estimator\": KNeighborsClassifier(),\n",
    "    \"hyperparameters\":\n",
    "        {\n",
    "            \"n_neighbors\": range(1,20,2),\n",
    "            \"weights\": [\"distance\", \"uniform\"],\n",
    "            \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "            \"p\": [1,2]\n",
    "        }\n",
    "}\n",
    "We can then use a for loop to iterate over the list of dictionaries, and for each one we can use scikit-learn's model_selection.GridSearchCV [https://www.dataquest.io/m/188/guided-project%3A-creating-a-kaggle-workflow/6/scikit-learn.org/0.18/modules/generated/sklearn.model_selection.GridSearchCV.html] class to find the best set of performing parameters, and add values for both the parameter set and the score to the dictionary.\n",
    "\n",
    "Finally, we can return the list of dictionaries, which will have our trained GridSearchCV objects as well as the results so we can see which was the most accurate.\n",
    "\n",
    "Instructions\n",
    "Import model_selection.GridSearchCV, neighbors import KNeighborsClassifier, and linear_model import LogisticRegression\n",
    "Create a function, select_model(), that:\n",
    "Accepts a dataframe and a list of features as input\n",
    "Splits the dataframe into all_X (containing only the features in the input parameter) and all_y\n",
    "Contains a list of dictionaries, each containing a model name, its estimator and a dictionary of hyperparameters:\n",
    "LogisticRegression, using the following hyperparameters:\n",
    "\"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"\n",
    "KNeighborsClassifier, using the following hyperparameters:\n",
    "\"n_neighbors\": range(1,20,2)\n",
    "\"weights\": [\"distance\", \"uniform\"]\n",
    "\"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "\"p\": [1,2]\n",
    "RandomForestClassifier, using the following hyperparameters:\n",
    "\"n_estimators\": [4, 6, 9]\n",
    "\"criterion\": [\"entropy\", \"gini\"]\n",
    "\"max_depth\": [2, 5, 10]\n",
    "\"max_features\": [\"log2\", \"sqrt\"]\n",
    "\"min_samples_leaf\": [1, 5, 8]\n",
    "\"min_samples_split\": [2, 3, 5]\n",
    "Iterate over that list of dictionaries, and for each dictionary:\n",
    "Print the name of the model.\n",
    "Instantiate a GridSearchCV() object using the model, the dictionary of hyperparameters and specify 10 fold cross validation.\n",
    "Fit the GridSearchCV() object using all_X and all_y.\n",
    "Assign the parameters and score for the best model to the dictionary.\n",
    "Assign the best estimator for the best model to the dictionary.\n",
    "Print the the parameters and score for the best model.\n",
    "Return the list of dictionaries\n",
    "Run the newly created function using the train dataframe and the output of select_features() as inputs and assign the result to a variable."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7. Making a Submission to Kaggle\n",
    "\n",
    "After running your function, you will have three scores from three different models. At this point in the workflow you have a decision to make: Do you want to train your best model on the holdout set and make a Kaggle submission, or do you want to go back to engineering features.\n",
    "\n",
    "You may find that adding a feature to your model doesn't improve your accuracy. In that case you should go back to data exploration and repeat the cycle again.\n",
    "\n",
    "If you're going to be continually submitting to Kaggle, a function will help make this easier. Let's create a function to automate this.\n",
    "\n",
    "Note that in our Jupyter Notebook environment, the DataFrame.to_csv() method will save the CSV in the same directory as your notebook, just as it would if you are running Jupyter locally. To download the CSV from our environment, you can either click the 'download' button to download all of your project files as a tar file, or click the Jupyter logo at the top of the interface, and navigate to the CSV itself to download just that file.\n",
    "\n",
    "Instructions\n",
    "Create a function, save_submission_file(), that:\n",
    "Accepts a trained model and a list of columns as required arguments, and an optional filename argument\n",
    "Uses the model to make predictions on the holdout dataframe using the columns specified.\n",
    "Transforms the predictions into a submission dataframe with PassenderID and Survived columns as specified by Kaggle\n",
    "Saves that dataframe to a CSV file with either a default filename, or the filename specified by the optional argument\n",
    "Retrieve the best performing model from the variable returned by select_model().\n",
    "Use save_submission_file() to save out a CSV of predictions.\n",
    "Download that file and submit it to Kaggle."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8. Next Steps\n",
    "\n",
    "In this guided project, we created a reproducible workflow to help us iterate over ideas and continue to improve the accuracy of our predictions. We also created helper functions which will make feature selection, model selection/tuning and creating submissions much easier as we continue to explore the data and create new features.\n",
    "\n",
    "Kaggle Workflow\n",
    "https://s3.amazonaws.com/dq-content/188/kaggle_workflow.svg\n",
    "\n",
    "We encourage you to continue working on this Kaggle competition. Here are some suggestions of next steps:\n",
    "\n",
    "Continue to explore the data and create new features, following the workflow and using the functions we created.\n",
    "Read more about the titanic and this Kaggle competition to get ideas for new features.\n",
    "Use some different algorithms in the select_model() function, like support vector machines, stochastic gradient descent or perceptron linear models.\n",
    "Experiment with RandomizedSearchCV instead of GridSearchCV to speed up your select_features() function.\n",
    "You can continue to work on this Kaggle competition within this guided project environment and save out files for submission if you like, although we would encourage you to set up your own Python environment so that you can work on your own computer. We have a Python Installation Guide that walks you through how to do this.\n",
    "\n",
    "Lastly, while the Titanic competition is great for learning about how to approach your first Kaggle competition, we recommend against spending many hours focused on trying to get to the top of the leaderboard. With such a small data set, there is a limit to how good your predictions can be, and your time would be better spent moving onto more complex competitions.\n",
    "\n",
    "Once you feel like you have a good understanding of the Kaggle workflow, you should look at some other competitions - a great next competition is the House Prices Competition. We have a great tutorial for getting started with this competition on our blog. [https://www.dataquest.io/blog/kaggle-getting-started/] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
