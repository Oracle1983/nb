{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 Advanced Topics in Data Science - Intro to Natural Language Processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "When you hear your native language, you intuitively know the meaning of what you heard. However, many people who've tried to learn a second or third language find the process to be much more painful. They have to break the language down into components like tenses in order to understand it better. Many have to take years of language lessons to get to the point where they can have a conversation.\n",
    "\n",
    "Learning a language is difficult because language has many complex rules. If we want computers to be able to understand language, we either need to explicitly teach computers the rules, or enable the computers to intuit the rules themselves. The former is a lot like learning a second language, and the latter is a lot like learning your native language.\n",
    "\n",
    "Broadly speakingly, natural language processing is the study of enabling computers to understand human languages. This field may involve teaching computers to automatically score essays, infer grammatical rules, or determine the emotions associated with text.\n",
    "\n",
    "In this mission, we'll learn some of the basic building blocks of natural langage processing. When we feed a computer written text, it has no idea what that text means. In order for a computer to begin making inferences from it, we'll need to convert the text to a numerical representation. This process will enable the computer to intuit grammatical rules, which is more akin to learning a first language.\n",
    "\n",
    "We'll explore how to get from written text to a numerical representation, and how we can use that representation to make predictions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Overview of the Data\n",
    "\n",
    "Hacker News is a community where users can submit articles, and other users can upvote those articles. The articles with the most upvotes make it to the front page, where they're more visible to the community.\n",
    "\n",
    "Our data set consists of submissions users made to Hacker News from 2006 to 2015. Developer Arnaud Drizard used the Hacker News API to scrape the data, which you can find in one of his GitHub repositories [https://github.com/arnauddri/hn]. We've sampled 3000 rows from the data randomly, and removed all of the extraneous columns. Our data only has four columns:\n",
    "\n",
    "submission_time - When the article was submitted\n",
    "upvotes - The number of upvotes the article received\n",
    "url - The base URL of the article\n",
    "headline - The article's headline\n",
    "In this mission, we'll be predicting the number of upvotes the articles received, based on their headlines. Because upvotes are an indicator of popularity, we'll discover which types of articles tend to be the most popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submissions = pd.read_csv(\"sel_hn_stories.csv\")\n",
    "submissions.columns = [\"submission_time\", \"upvotes\", \"url\", \"headline\"]\n",
    "submissions = submissions.dropna()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Tokenizing the Headlines\n",
    "\n",
    "Our goal is to train a linear regression algorithm that predicts the number of upvotes a headline would receive. To do this, we'll need to convert each headline to a numerical representation.\n",
    "\n",
    "While there are several ways to accomplish this, we'll use a bag of words model. A bag of words model [https://en.wikipedia.org/wiki/Bag-of-words_model] represents each piece of text as a numerical vector.\n",
    "\n",
    "We'll examine each step in the bag of words process in this mission. For now, here's a high-level diagram showing how two sentences, I rode my horse to Berlin. and You rode my horse to Berlin in the winter., convert to a bag of words:\n",
    "\n",
    "1 I rode my horse to Berlin.\n",
    "2 You rode my horse to Berlin in the winter.\n",
    "\n",
    "(Bag of words)\n",
    "    i  you  rode  my  horse  to  berlin  in  the\n",
    "1   1   0    1     1    1     1    1      0    0  \n",
    "2   0   1    1     1    1     1    1      1    1\n",
    "\n",
    "The first step in creating a bag of words model is tokenization. In tokenization [https://en.wikipedia.org/wiki/Tokenization], we break a sentence up into disconnected words.\n",
    "\n",
    "Here's a diagram in which we tokenize the two sentences we mentioned above:\n",
    "\n",
    "1 I rode my horse to Berlin.\n",
    "2 You rode my horse to Berlin in the winter.\n",
    "\n",
    "(Tokenization)\n",
    "    \n",
    "1   [I, rode, myt, horse, to, Berlin.]\n",
    "2   [You, rode, my, horse, to, Berlin, in, the, winter]\n",
    "\n",
    "As you can see, all we're doing is splitting each sentence into a list of individual words, or tokens. The split occurs on the space character (\" \").\n",
    "\n",
    "Instructions\n",
    "Split each headline into individual words on the space character(\" \"), and append the resulting list to tokenized_headlines.\n",
    "When you're finished, tokenized_headlines should be a list of lists. Each list should contain the tokens for the headline located at the corresponding position in the submissions dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_headlines = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Preprocessing Tokens to Increase Accuracy\n",
    "\n",
    "We now have tokens, but we need to process them a bit to make our predictions more accurate. We know that Berlin, Berlin., and berlin all refer to the same word, but the computer doesn't know that. We'll need to convert those variations so that they're consistent.\n",
    "\n",
    "We can do this by lowercasing (which will convert Berlin to berlin), and also by removing punctuation (so Berlin. becomes Berlin).\n",
    "\n",
    "Preprocessing doesn't have to be perfect, but the more we can help the computer group the same word together, the higher our prediction accuracy will be. Take a look through your tokens, and see if there are any instances of the same word that you haven't grouped together.\n",
    "\n",
    "Instructions\n",
    "Loop through each item in tokenized_headlines, which is a list of lists.\n",
    "For each list of tokens:\n",
    "Convert each individual token to lowercase\n",
    "Remove all of the items in the punctuation list from each individual token\n",
    "Append the clean list to clean_tokenized\n",
    "clean_tokenized should now be a list of lists. Each list should contain the preprocessed tokens associated with the headline in the corresponding position of the submissions dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = [\",\", \":\", \";\", \".\", \"'\", '\"', \"â€™\", \"?\", \"/\", \"-\", \"+\", \"&\", \"(\", \")\"]\n",
    "clean_tokenized = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. Assembling a Matrix of Unique Words\n",
    "\n",
    "Now that we have our tokens, we can begin converting the sentences to their numerical representations. First, we'll retrieve all of the unique words from all of the headlines. Then, we'll create a matrix, and assign those words as the column headers. We'll initialize all of the values in the matrix to 0.\n",
    "\n",
    "We'll use a pandas dataframe instead of a NumPy matrix. We can create a dataframe with all zero values using this syntax:\n",
    "\n",
    "pd.DataFrame(0, index=np.arange(len(clean_tokenized)), columns=unique_tokens)\n",
    "The code above will create a dataframe with as many rows as the number of items in clean_tokenized. Each column name will be a word from unique_tokens. This assumes that we already assigned the unique tokens to unique_tokens. Each cell in the dataframe will have the value 0. You can find more documentation on initializing a dataframe in the pandas documentation.\n",
    "\n",
    "Instructions\n",
    "Find all of the unique tokens in clean_tokenized, and assign the result to unique_tokens.\n",
    "Only add tokens that occur more than once (across all of the headlines). Tokens that only occur once don't add anything to the model's prediction power, and removing them will make our algorithm run much more quickly.\n",
    "To do this, you can keep a list of the tokens that occur once in the data, and a different list of the tokens that occur more than once. If a token is already in the first list when you encounter it and it's not in the second list, you should add it to the second list.\n",
    "When you're finished, unique_tokens should contain any tokens that occur more than once across all of the headlines.\n",
    "Each token in unique_tokens should only appear in the list a single time.\n",
    "Create a dataframe with as many rows as there are items in the clean_tokenized list. Each column name should be a token in unique_tokens. Initialize all of the cells to the value 0. Assign the dataframe to the variable counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unique_tokens = []\n",
    "single_tokens = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. Counting Token Occurrences\n",
    "\n",
    "Now that we have a matrix where all values are 0, we need to fill in the correct counts for each cell. This involves going through each set of tokens, and incrementing the column counters in the appropriate row.\n",
    "\n",
    "When we're finished, we'll have a row vector for each headline that tells us how many times each token occured in that headline.\n",
    "\n",
    "To accomplish this, we can loop through each list of tokens in clean_tokenized, then loop through each token in the list and increment the proper cell.\n",
    "\n",
    "Instructions\n",
    "Loop through each list of tokens in clean_tokenized.\n",
    "You should use the enumerate() function when writing the loop to get an index along with the list of tokens.\n",
    "Loop through each token in the list of tokens.\n",
    "Check whether the token is in unique_tokens. If not, it isn't a column in the dataframe, and you should ignore it.\n",
    "Increment the appropriate cell by indexing the row of counts, and finding the right column for the token. Add 1 to the cell to indicate that you found the token once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We've already loaded in clean_tokenized and counts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7. Removing Columns to Increase Accuracy\n",
    "\n",
    "We have over 2000 columns in our matrix. This can make it very hard for a linear regression model to make good predictions. Too many columns will cause the model to fit to noise instead of the signal in the data.\n",
    "\n",
    "There are two kinds of features that will reduce prediction accuracy. Features that occur only a few times will cause overfitting, because the model doesn't have enough information to accurately decide whether they're important. These features will probably correlate differently with upvotes in the test set and the training set.\n",
    "\n",
    "Features that occur too many times can also cause issues. These are words like and and to, which occur in nearly every headline. These words don't add any information, because they don't necessarily correlate with upvotes. These types of words are sometimes called stopwords.\n",
    "\n",
    "To reduce the number of features and enable the linear regression model to make better predictions, we'll remove any words that occur fewer than 5 times or more than 100 times.\n",
    "\n",
    "Instructions\n",
    "Generate a vector that contains the sum of each column in counts. This data will indicate how many times each word occurs in the headlines. You can use the sum() method on pandas dataframes to accomplish this. Assign this vector to word_counts.\n",
    "Use the vector to filter counts to remove any columns that occur less than 5 times, or more than 100 times. You can use the loc method on dataframes to accomplish this."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8. Splitting the Data Into Train and Test Sets\n",
    "\n",
    "Now we'll need to split the data into two sets so that we can evaluate our algorithm effectively. We'll train our algorithm on a training set, then test its performance on a test set.\n",
    "\n",
    "The train_test_split() [http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html] function from scikit-learn will help us accomplish this.\n",
    "\n",
    "We'll pass in .2 for the test_size parameter to randomly select 20% of the rows for our test set, and 80% for our training set.\n",
    "\n",
    "X_train and X_test contain the predictors, and y_train and y_test contain the value we're trying to predict (upvotes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, submissions[\"upvotes\"], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "9. Making Predictions With fit()\n",
    "\n",
    "Now that we have a training set and a test set, let's train a model and make test predictions. We'll use a linear regression algorithm from scikit-learn, which you can read more about in the scikit-learn documentation. [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html]\n",
    "\n",
    "First we'll initialize the model using the LinearRegression class. Then, we'll use the fit() method on the model to train with X_train and y_train. Finally, we'll make predictions with X_test.\n",
    "\n",
    "When we make predictions with a linear regression model, the model assigns coefficients to each column. Essentially, the model is determining which words correlate with more upvotes, and which with less. By finding these correlations, the model will be able to predict which headlines will be highly upvoted in the future. While the algorithm won't have a high level of understanding of the text, linear regression can generate surprisingly good results.\n",
    "\n",
    "Instructions\n",
    "Train clf using the fit() method.\n",
    "Use the predict() method on clf to make predictions on X_test. Assign the result to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf = LinearRegression()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "10. Calculating Prediction Error\n",
    "\n",
    "Now that we have predictions, we can calculate our prediction error. We'll need to select an error metric first, though. We'll use mean squared error (MSE), which is a common error metric.\n",
    "\n",
    "Here's the formula for MSE:\n",
    "\n",
    "\n",
    "With MSE, we subtract the predictions from the actual values, square the results, and find the mean. Because the errors are squared, MSE penalizes errors further away from the actual value more than those close to the actual value. We want to use MSE because we'd like all of our predictions to be relatively close to the actual values.\n",
    "\n",
    "Instructions\n",
    "Calculate the mean squared error associated with our predictions.\n",
    "Subtract y_test from predictions.\n",
    "Square each of the differences.\n",
    "Add all of the squared differences together, and divide by the number of differences to get the mean.\n",
    "Assign the result to mse."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11. Next Steps\n",
    "\n",
    "Our MSE is 2181, which is a fairly large value. There's no hard and fast rule about what a \"good\" error rate is, because it depends on the problem we're solving and our error tolerance.\n",
    "\n",
    "In this case, the mean number of upvotes is 10, and the standard deviation is 39.5. If we take the square root of our MSE to calculate error in terms of upvotes, we get 46.7. This means that our average error is 46.7 upvotes away from the true value. This is higher than the standard deviation, so our predictions are often far off-base.\n",
    "\n",
    "We can take several steps to reduce the error and explore natural language processing further. Here are some ideas for your next steps:\n",
    "\n",
    "Use the entire data set. While we used samples in this mission, you could download the entire data set from this GitHub repository [https://github.com/arnauddri/hn]. This approach will reduce the error rate dramatically. There are many features in natural language processing. Using more data will ensure that the model will find more occurrences of the same features in the test and training sets, which will help the model make better predictions.\n",
    "Add \"meta\" features like headline length and average word length.\n",
    "Use a random forest, or another more powerful machine learning technique.\n",
    "Explore different thresholds for removing extraneous columns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
