{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Machine Learning - Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "In the last mission, we explored an iterative technique for model fitting named gradient descent. The gradient descent algorithm requires multiple iterations to converge on the optimal parameter values and the number of iterations is highly dependent on the initial parameter values and the learning rate we select.\n",
    "\n",
    "In this mission, we'll explore a technique called ordinary least squares estimation or OLS estimation for short. Unlike gradient descent, OLS estimation provides a clear formula to directly calculate the optimal parameter values that minimizes the cost function. To understand OLS estimation, we need to first frame our linear regression problem in the matrix form. We've mostly worked with the following form of the linear regression model:\n",
    "\n",
    "\n",
    "\n",
    "While this form represents the relationship between the features (\n",
    " to \n",
    ") and the target column (\n",
    ") well when there are just a few parameter values, it doesn't scale well to when we have hundreds of parameters. If you recall from the Linear Algebra for Machine Learning course, we explored how matrix notation lets us better represent and reason about a linear system with many variables. With that in mind, here's what the matrix form of our linear regression model looks like:\n",
    "\n",
    "\n",
    "Where \n",
    " is a matrix representing the columns from the training set our model uses, \n",
    " is a vector representing the parameter values, and \n",
    " is the vector of predictions. Here's a diagram with some sample values for each:\n",
    "\n",
    "Matrix Form\n",
    "\n",
    "Now that we've gained an understanding for the matrix representation of the linear regression model, let's take a peek at the OLS estimation formula that results in the optimal vector \n",
    ":\n",
    "\n",
    "\n",
    "Let's start by computing OLS estimation to find the best parameters for a model using the following features:\n",
    "\n",
    "features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area',\n",
    "       'Gr Liv Area', 'Overall Qual']\n",
    "In the following screens, we'll dive into the mathematical derivation of the OLS estimation technique. It's important to note that you'll most likely never implement this technique in a data science role and will instead use an existing, efficient implementation (scikit-learn uses OLS under the hood when you call fit() on a LinearRegression instance) [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html].\n",
    "\n",
    "Instructions\n",
    "Select just the features in features from the training set and assign to X.\n",
    "Select the SalePrice column from the training set and assign to y.\n",
    "Use the OLS estimation formula to return the optimal parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area',\n",
    "       'Gr Liv Area', 'Overall Qual']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Cost Function\n",
    "\n",
    "Unlike gradient descent, OLS estimation provides what is known as a closed form solution to the problem of finding the optimal parameter values. A closed form solution is one where a solution can be computed arithmetically with a predictable amount of mathematical operations. Gradient descent, on the other hand, is an algorithmic approach that can require a different number of iteration (and therefore a different number of mathematical operations) based on the initial parameter values, the learning rate, etc. While the approach is different, both techniques share the high level objective of minimizing the cost function.\n",
    "\n",
    "Before we can dive into how the cost function is represented in the matrix form, let's understand how the error is represented. Because the error is the difference between the predictions made using the model \n",
    " and the actual labels \n",
    ", it's represented as a vector. The greek letter for E (epsilon \n",
    ") is often used to represent the error vector:\n",
    "\n",
    "\n",
    "We can build on this to define \n",
    ":\n",
    "\n",
    "\n",
    "Even though this closely resembles the matrix equation of \n",
    ", we have 2 unknowns (the vector \n",
    " and the vector \n",
    ". We're looking for a model, represented using the parameter vector \n",
    ", that will minimize the mean squared error between the labels, \n",
    ", and the predictions, \n",
    ". Said another way, the cost function is this mean squared error.\n",
    "\n",
    "Here's what the cost function looks like in matrix form:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Derivative Of The Cost Function\n",
    "\n",
    "In mathematical optimization, a function that we optimize through minimization is known as a cost function or sometime as the loss function [https://en.wikipedia.org/wiki/Loss_function]. Because we're trying to fit a single parameter model, we can replace with \n",
    " with \n",
    " in the cost function:\n",
    "\n",
    "\n",
    "In this screen, we'll apply calculus properties to simplify this derivative to something we can compute. We encourage you to follow along using pencil and paper, and see if you can apply the properties we mention at each step to obtain the same result we did. Note that while you'll probably never have to implement gradient descent yourself (as most packages have high performance implementations), understanding the math will help make it easier for you to debug when you run into issues.\n",
    "\n",
    "\n",
    "By applying the linearity of differentiation property [https://en.wikipedia.org/wiki/Linearity_of_differentiation] from calculus, we can bring the derivative term inside the summation:\n",
    "\n",
    "\n",
    "\n",
    "We can apply both the power rule and the chain rule to simplify this. You can read more about the chain rule here [https://en.wikipedia.org/wiki/Chain_rule] or observe how both are applied together here[https://www.khanacademy.org/math/calculus-home/taking-derivatives-calc/chain-rule-calc/v/differentiating-powers-of-functions]:\n",
    "\n",
    "\n",
    "Because we're differentiating \n",
    " with respect to \n",
    ", we treat \n",
    " and \n",
    " as constants. \n",
    " then simplifies to just \n",
    ":\n",
    "\n",
    "\n",
    "For every iteration of gradient descent:\n",
    "\n",
    "this derivative is computed using the current \n",
    "value\n",
    "the derivative is multiplied by the learning rate (\n",
    "): \n",
    "the result is subtracted from the current parameter value and assigned as the new parameter value: \n",
    "Here's what this would look like in code if we ran gradient descent for 10 iterations:\n",
    "\n",
    "a1_list = [1000]\n",
    "alpha = 10\n",
    "â€‹\n",
    "for x in range(0, 10):\n",
    "    a1 = a1_list[x]\n",
    "    deriv = derivative(a1, alpha, xi_list, yi_list)\n",
    "    a1_new = a1 - alpha*deriv\n",
    "    a1_list.append(a1_new)\n",
    "To test your understanding, implement the derivative() function.\n",
    "\n",
    "Instructions\n",
    "Finish implementing the derivative() function:\n",
    "This function should return the derivative at the current value of \n",
    ".\n",
    "Uncomment the 2 lines of code that run the gradient_descent() function, assign the list of iterations for the \n",
    " parameter to param_iterations, and assign the last iteration for \n",
    " to final_param."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Derivative Of The Cost Function\n",
    "\n",
    "The derivative of the cost function is decently involved, and out of scope for this mission. Understanding the derivation requires some familiarity with matrix calculus [https://en.wikipedia.org/wiki/Matrix_calculus], which is a specific notation for applying calculus concepts to matrices. If you're interested in the derivation, we recommend that you read Eli Bendersky's wonderful walkthrough of the derivation on his blog[http://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/].\n",
    "\n",
    "Here's the derivative of the cost function:\n",
    "\n",
    "\n",
    "To find the vector \n",
    " that minmizes the cost function \n",
    ", we need to set the derivative equal to \n",
    " and solve for \n",
    ":\n",
    "\n",
    "\n",
    "Let's move the second term to the right hand side and divide both sides by \n",
    ":\n",
    "\n",
    "\n",
    "Our goal is to isolate \n",
    ", the parameter vector. The last step we need to perform is \"divide out\" \n",
    " from the left hand side.\n",
    "\n",
    "If you recall, we can \"divide\" matrix terms by computing the inverse. Let's dig up the example we explored in the linear algebra course. We can cancel \n",
    " from the following equation \n",
    " by multiplying both sides by the inverse \n",
    ". This leaves us with \n",
    " .\n",
    "\n",
    "To cancel \n",
    " from the left side, we need to compute the inverse of it and multiply it by both sides. We're now left with the OLS estimation formula:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Gradient Descent vs. Ordinary Least Squares\n",
    "\n",
    "Now that we've explored a lot of the math that underlies OLS estimation, let's understand its limitations. The biggest limitation is that OLS estimation is computationally expensive when the data is large. This is because computing a matrix inverse has a computational complexity of apprximately O(n^3). You can read more about computational complexity of the matrix inverse and other common matrix operations on Wikipedia.\n",
    "\n",
    "OLS is commonly used when the number of elements in the dataset (and therefore the matrix that's inverted) is less than a few million elements. On larger datasets, gradient descent is used because it's much more flexible. For many practical problems, we can set a threshold accuracy value (or a set number of iterations) and use a \"good enough\" solution. This is especially useful when iterating and trying different features in our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
