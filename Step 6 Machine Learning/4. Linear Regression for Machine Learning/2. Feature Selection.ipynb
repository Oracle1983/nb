{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Machine Learning -  Feature Selection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Missing Values\n",
    "\n",
    "In the machine learning workflow, once we've selected the model we want to use, selecting the appropriate features for that model is the next important step. In this mission, we'll explore how to use correlation between features and the target column, correlation between features, and variance of features to select features. We'll continue working with the same housing dataset from the last mission.\n",
    "\n",
    "We'll specifically focus on selecting from feature columns that don't have any missing values or don't need to be transformed to be useful (e.g. columns like Year Built and Year Remod/Add). We'll explore how to deal with both of these in a later mission in this course.\n",
    "\n",
    "To start, let's look at which columns fall into either of these two categories.\n",
    "\n",
    "Instructions\n",
    "Drop the following columns from numerical_train:\n",
    "PID (place ID isn't useful for modeling)\n",
    "Year Built\n",
    "Year Remod/Add\n",
    "Garage Yr Blt\n",
    "Mo Sold\n",
    "Yr Sold\n",
    "Calculate the number of missing values from each column in numerical_train. Create a Series object where the index is made up of column names and the associated values are the number of missing values:\n",
    "Order                0\n",
    "PID                  0\n",
    "MS SubClass          0\n",
    "MS Zoning            0\n",
    "...\n",
    "Assign this Series object to null_series. Select the subset of null_series to keep only the columns with no missing values, and assign the resulting Series object to full_cols_series.\n",
    "Display full_cols_series using the print() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "numerical_train = train.select_dtypes(include=['int', 'float'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Correlating Feature Columns With Target Column\n",
    "\n",
    "In the last mission, we selected the feature for the simple linear regression model by comparing how some of the features correlate with the target column. If you recall, we focused on 4 features in particular and used the pandas.DataFrame.corr() method to return the correlation coefficients between each pairs of columns. This means that the correlation matrix for 4 columns results in 16 correlation values:\n",
    "\n",
    ">>> train[['GarageArea', 'GrLivArea', 'OverallCond', 'SalePrice']].corr()\n",
    "             GarageArea  GrLivArea  OverallCond  SalePrice\n",
    "GarageArea     1.000000   0.468997    -0.151521   0.623431\n",
    "GrLivArea      0.468997   1.000000    -0.079686   0.708624\n",
    "OverallCond   -0.151521  -0.079686     1.000000  -0.077856\n",
    "SalePrice      0.623431   0.708624    -0.077856   1.000000\n",
    "The subset of features we want to focus on, full_cols_series, contains 27 columns:\n",
    "\n",
    ">>> len(full_cols_series)\n",
    "27\n",
    "The resulting correlation matrix will contain 27 * 27 or 729 correlation values. Comparing and contrasting this many values is incredibly difficult. Let's instead focus on just how the feature columns correlate with the target column (SalePrice) instead.\n",
    "\n",
    "Instructions\n",
    "Compute the pairwise correlation coefficients between all of the columns in train_subset.\n",
    "Select just the SalePrice column from the resulting data frame, compute the absolute value of each term, sort the resulting Series by the correlation values, and assign to sorted_corrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_subset = train[full_cols_series.index]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Correlation Matrix Heatmap\n",
    "\n",
    "We now have a decent list of candidate features to use in our model, sorted by how strongly they're correlated with the SalePrice column. For now, let's keep only the features that have a correlation of 0.3 or higher. This cutoff is a bit arbitrary and, in general, it's a good idea to experiment with this cutoff. For example, you can train and test models using the columns selected using different cutoffs and see where your model stops improving.\n",
    "\n",
    "The next thing we need to look for is for potential collinearity between some of these feature columns. Collinearity is when 2 feature columns are highly correlated and stand the risk of duplicating information. If we have 2 features that convey the same information using 2 different measures or metrics, we don't need to keep both.\n",
    "\n",
    "While we can check for collinearity between 2 columns using the correlation matrix, we run the risk of information overload. We can instead generate a correlation matrix heatmap using Seaborn to visually compare the correlations and look for problematic pairwise feature correlations. Because we're looking for outlier values in the heatmap, this visual representation is easier.\n",
    "\n",
    "Here's what the example correlation matrix heatmap looks like from the documentation:\n",
    "\n",
    "Correlation Heatmap Matrix\n",
    "https://s3.amazonaws.com/dq-content/236/correlation_heatmap_matrix.png\n",
    "\n",
    "To generate a correlation matrix heatmap, we need to pass in the data frame containing the correlation matrix as a data frame into the seaborn.heatmap() [http://seaborn.pydata.org/examples/heatmap_annotation.html] function.\n",
    "\n",
    "Instructions\n",
    "Select only the columns in sorted_corrs with a correlation above 0.3 and assign to strong_corrs.\n",
    "Use the seaborn.heatmap() [http://seaborn.pydata.org/generated/seaborn.heatmap.html] function to generate a correlation matrix heatmap for the columns in strong_corrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Train And Test Model\n",
    "\n",
    "Based on the correlation matrix heatmap, we can tell that the following pairs of columns are strongly correlated:\n",
    "\n",
    "Gr Liv Area and TotRms AbvGrd\n",
    "Garage Area and Garage Cars\n",
    "If we read the descriptions of these columns from the data documentation [https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt], we can tell that each pair of column reflects very similar information. Bceause Gr Liv Area and Garage Area are continuous variables that capture more nuance, let's drop the TotRms AbvGrd and Garage Cars.\n",
    "\n",
    "The last thing we'll need to do is confirm that the test set contains no missing values for these columns:\n",
    "\n",
    ">>> final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])\n",
    ">>> test[final_corr_cols.index]\n",
    "class 'pandas.core.frame.DataFrame'\n",
    "RangeIndex: 1470 entries, 1460 to 2929\n",
    "Data columns (total 9 columns):\n",
    "Wood Deck SF     1470 non-null int64\n",
    "Open Porch SF    1470 non-null int64\n",
    "Fireplaces       1470 non-null int64\n",
    "Full Bath        1470 non-null int64\n",
    "1st Flr SF       1470 non-null int64\n",
    "Garage Area      1469 non-null float64\n",
    "Gr Liv Area      1470 non-null int64\n",
    "Overall Qual     1470 non-null int64\n",
    "SalePrice        1470 non-null int64\n",
    "dtypes: float64(1), int64(8)\n",
    "memory usage: 103.4 KB\n",
    "Looks like the test set has one pesky row with a missing value for Garage Area. Let's just drop this row for now. Finally, let's train and test a model using these columns to see how they fare.\n",
    "\n",
    "Instructions\n",
    "Filter the test data frame so it only contains the columns from final_corr_cols.index. Then, drop the row containing missing values and assign the result to clean_test\n",
    "Build a linear regression model using the features in features.\n",
    "Calculate the RMSE on the test and train sets.\n",
    "Assign the train RMSE to train_rmse and the test RMSE to test_rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])\n",
    "features = final_corr_cols.drop(['SalePrice']).index\n",
    "target = 'SalePrice'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. Removing Low Variance Features\n",
    "\n",
    "The last technique we'll explore is removing features with low variance. When the values in a feature column have low variance, they don't meaningfully contribute to the model's predictive capability. On the extreme end, let's imagine a column with a variance of 0. This would mean that all of the values in that column were exactly the same. This means that the column isn't informative and isn't going to help the model make better predictions.\n",
    "\n",
    "To make apples to apples comparisons between columns, we need to rescale all of the columns to vary between 0 and 1. Then, we can set a cutoff value for variance and remove features that have less than that variance amount. This is known as min-max scaling or as rescaling. Here's the formula for rescaling:\n",
    "\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "\n",
    " is an individual value\n",
    "\n",
    " is the minimum value for the column \n",
    "belongs to\n",
    "\n",
    " is the maximum valu3 for the column \n",
    "belongs to\n",
    " \n",
    "Instructions\n",
    "Select the columns in features from the train data frame. Rescale each of the columns so the values range from 0 to 1.\n",
    "Calculate and display the column minimum and maximum values to ensure that all values range from 0 to 1.\n",
    "Calculate the variance of these columns, sort the resulting series by its values, and assign to sorted_vars.\n",
    "Display sorted_vars using the print() function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. Final Model\n",
    "\n",
    "To wrap up this mission, let's set a cutoff variance of 0.015, remove the Open Porch SF feature, and train and test a model using the remaining features.\n",
    "\n",
    "Instructions\n",
    "Build a linear regression model using the remaining features.\n",
    "Calculate the RMSE on the test and train sets.\n",
    "Assign the train RMSE to train_rmse_2 and the test RMSE to test_rmse_2.\n",
    "Display both RMSE values using the print() function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "7. Next Steps\n",
    "\n",
    "We were able to improve the RMSE value to approximately 40591 by removing the Open Porch SF feature. This is most likely the furthest we can go without transforming and utilizing the other features in the dataset so we'll stop here for now. In the next 2 missions, we'll explore 2 different ways of fitting models. Afterwards, we'll explore ways to clean and engineer new features from the existing features to improve model accuracy even further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
