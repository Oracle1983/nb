{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 The Command Line - Guided Project Transforming data with Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. How guided projects work\n",
    "\n",
    "Welcome to the first Dataquest guided project! Guided projects are a way to help you synthesize concepts learned during the Dataquest missions, and start building a portfolio. Guided projects go above and beyond regular projects by providing an in-browser coding experience, along with help and hints. Guided projects bridge the gap between learning using the Dataquest missions, and applying the knowledge on your own computer.\n",
    "\n",
    "Guided projects help you develop key skills that you'll need to perform data science work in the \"real world\". Doing well on these projects is slightly different from doing well in the missions, where there is a \"right\" answer. In the guided projects, you'll need to think up and create solutions on your own (although we'll be there to help along the way).\n",
    "\n",
    "The guided project interface is structured much like an IDE on your local machine would be. This area contains text and instructions. You can advance between steps in the project whenever you want -- since there's no \"right\" answer for any screen, the text is mainly for you to use as a reference and guide as you build the project. To the right is a file browser interface, where you can view, create, and edit files. Under the file browser is a terminal window, where you can run shell commands.\n",
    "\n",
    "Note: Only files stored in the project folder, in this case /home/dq/scripts, will be saved! If you make changes to files elsewhere, they won't be saved.\n",
    "\n",
    "As you go through this project, Google, StackOverflow, and the documentation for various packages will help you along the way. All data scientists make extensive use of these and other resources as they write code, and so should you.\n",
    "\n",
    "We'd love to hear your feedback as you go through this project, and we hope it's a great experience!\n",
    "\n",
    "Instructions\n",
    "For now, just hit \"Next\" to get started with the project!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. The dataset\n",
    "\n",
    "In this project, you'll be working with a dataset of submissions to Hacker News from 2006 to 2015. Hacker News is a site where users can submit articles from across the internet (usually about technology and startups), and others can \"upvote\" the articles, signifying that they like them. The more upvotes a submission gets, the more popular it was in the community. Popular articles get to the \"front page\" of Hacker News, where they're more likely to be seen by others.\n",
    "\n",
    "The dataset you'll be using was compiled by Arnaud Drizard using the Hacker News API, and can be found here [https://github.com/arnauddri/hn]. We've sampled 10000 rows from the data randomly, and removed all extraneous columns. Our dataset only has four columns:\n",
    "\n",
    "submission_time -- when the story was submitted.\n",
    "upvotes -- number of upvotes the submission got.\n",
    "url -- the base domain of the submission.\n",
    "headline -- the headline of the submission. Users can edit this, and it doesn't have to match the headline of the original article.\n",
    "You'll be writing scripts to answer some main questions:\n",
    "\n",
    "What words appear most often in the headlines?\n",
    "What domains were submitted most often to Hacker News?\n",
    "At what times are the most articles submitted?\n",
    "You'll be answering these questions by writing command line scripts, instead of using IPython notebook. IPython notebooks are great for quick data visualization and exploration, but Python scripts are the way to put anything we learn into production. Let's say you want to make a website to help people write headlines that get as many upvotes as possible, and submit articles at the right time. To do this, you'll need scripts."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Reading the data\n",
    "\n",
    "There should be a file called read.py already open. You can run this from the command line by being in the same folder, and typing python read.py. Of course, there's nothing in the file right now. You might recall from the last mission that you can put this into a file to run it from the command line:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to a Python script\")\n",
    "This will print Welcome to a Python script on the command line if you put it into a file and run it.\n",
    "\n",
    "We can also add functions into a file by writing them like normal:\n",
    "\n",
    "def load_data():\n",
    "    pass\n",
    "â€‹\n",
    "    if __name__ == \"__main__\":\n",
    "        # This will call load_data if you run the script from the command line.\n",
    "        load_data()\n",
    "Function definitions should come before the if __name__ == \"__main__\" line. These functions can be imported from other files.\n",
    "\n",
    "We'll be adding some code to the read.py file that will help us load in the dataset and do some initial processing. We'll then be able to import the code to read in the dataset from other scripts we develop.\n",
    "\n",
    "Instructions\n",
    "In the read.py file, read the hn_stories.csv file into a Pandas Dataframe.\n",
    "There is no header row in the data, so the columns don't have names. See this stackoverflow thread [http://stackoverflow.com/questions/11346283/renaming-columns-in-pandas] for how to add column names. Add the column names from the last screen (submission_time, upvotes, url, and headline) to the Dataframe.\n",
    "Create a function called load_data that takes no inputs, but contains the code to read in and process the dataset. load_data should return a Pandas Dataframe with the column names set correctly.\n",
    "As you work on these steps, you should be running your script on the command line every so often and verifying that things are working. You can run read.py from the command line by calling python read.py. The first verification is to make sure that you don't see any errors. The second one is to call print at key points in your code, and make sure that the output looks like what you expect. You might want to do this after each step above. This is a good general rule of thumb to follow when writing new code."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Which words appear in the headlines often?\n",
    "\n",
    "We now want to figure out which words appear most often in the headlines. We'll be developing another script, called count.py to accomplish this. We'll need to import our load_data function from read.py into count.py so we can use it.\n",
    "\n",
    "You'll recall that if you have a folder with two files, read.py and count.py, you can use the function load_data in read.py from count.py by writing the following code in count.py:\n",
    "\n",
    "import read\n",
    "df = read.load_data()\n",
    "\n",
    "Instructions\n",
    "Writing the script for this will require a series of steps:\n",
    "Make a file called count.py, using the file browser, or the command line.\n",
    "Import load_data from read.py, and call the function to read in the dataset.\n",
    "The order in which you do the below two steps is up to you, but it's suggested to first combine all the headlines (you can use a for loop for this, among other methods), and then split everything into words.\n",
    "Combine all of the headlines together into one long string. You'll want to leave a space between each headline when you combine them. Here's a good reference on joining strings. [http://stackoverflow.com/questions/4435169/good-way-to-append-to-a-string]\n",
    "Figure out how to split the long string into words. Each headline is a string, such as Anticlimax As Motivation Killer. Combining that with Swype acquired by Nuance for 100 million would look like Anticlimax As Motivation Killer Swype acquired by Nuance for 100 million. Adding more headlines would make a longer string. You'll need to figure out a way to split the long string, so you end up with a list of words. The documentation for str might help here.\n",
    "You might want to think about lowercasing each word, so Hello and hello aren't treated as different words when you do a count.\n",
    "Find a way to count up how many times each word occurs in the list. The Counter [https://docs.python.org/3/library/collections.html#collections.Counter] class might help you.\n",
    "Add code to print the 100 words that occur the most in your data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. Which domains were submitted most often?\n",
    "\n",
    "You can now move on to our second question, and explore which domains were submitted most often. We'll want to make a separate script, called domains.py, for this.\n",
    "\n",
    "Instructions\n",
    "Here are the steps:\n",
    "Make a file called domains.py, using the file browser, or the command line.\n",
    "Add in the code to read the file hn_stories.csv, and add column names.\n",
    "You can think of each domain name as a \"word\". A domain will look like scala-lang.org, or blog.iweb.com.\n",
    "You can use the value_counts method in pandas to count the number of occurrences of each value in a column. Here are the docs. [http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.Series.value_counts.html]\n",
    "Print the 100 most submitted domains.\n",
    "By default, Pandas only prints 10 rows of a Dataframe or Series. There is a pandas option to make it print more rows (see this thread on Stackoverflow), but there are bugs with it and Series. Instead, just loop through the series and print the index value, and the total. Here's some sample code:\n",
    "for name, row in domains.items():\n",
    "    print(\"{0}: {1}\".format(name, row))\n",
    "The above code assumes that the results of running value_counts is assigned to domains.\n",
    "You can extend this analysis and make it a bit more robust by removing subdomains. For example, blog.iweb.com and iweb.com would be separate domains at the moment, but they are the same. By removing the subdomain, you can turn blog.iweb.com into iweb.com. You can remove the subdomain using the apply method on Pandas Series and Dataframes. Here's the documentation. [http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.apply.html]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. When are the most articles submitted?\n",
    "\n",
    "We want to know when the most articles are submitted. One easy way to reframe this is to look at what hour articles are submitted. To figure this out, we'll need to use the submission_time column.\n",
    "\n",
    "The submission_time column contains timestamps, which look like this: 2011-11-09T21:56:22Z. These times are expressed in UTC, which is a universal time zone used by most software for consistency (imagine a database populated with times all having different timezones; it would be a huge pain to work with).\n",
    "\n",
    "To get hour from a timestamp, we can use the dateutil library. The parser module in dateutil contains the parse function, which can take in a timestamp, and return a datetime object. Here's [https://dateutil.readthedocs.org/en/latest/parser.html] a link to the documentation. After parsing the timestamp, the hour property of the resulting date object will tell you the hour the article was submitted.\n",
    "\n",
    "Instructions\n",
    "Make a file called times.py to find the submission times.\n",
    "Write a function to extract the hour from a timestamp. This function should first use dateutil.parser.parse to parse the timestamp, then extract the hour from the resulting datetime object, then return the hour.\n",
    "Use the pandas apply method to make a column of submission hours.\n",
    "Use the value_counts method to find the number of occurences of each hour.\n",
    "Print out the results.\n",
    "You can repeat this procedure to find how many articles were submitted on each day of the month, year, minute, day of the week, and so on."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7. Next steps\n",
    "\n",
    "That's all for the guided steps, but feel free to keep going through the data and answering questions. We encourage you to think of your own questions, and to be creative in exploring the dataset!\n",
    "\n",
    "If you can't think of any questions, some interesting ones are:\n",
    "\n",
    "What headline length leads to the most upvotes?\n",
    "What submission time leads to the most upvotes?\n",
    "How are the total numbers of upvotes changing over time?\n",
    "You can write scripts and explore here, or download the code to your computer using the download icon to the right. You'll then be able to run the scripts on your own computer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
