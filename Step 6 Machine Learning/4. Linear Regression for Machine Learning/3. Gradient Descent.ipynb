{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Machine Learning - Gradient Descent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "In the previous missions, we learned how the linear regression model estimates the relationship between the feature columns and the target column and how we can use that for making predictions. In this mission and the next, we'll discuss the 2 most common ways for finding the optimal parameter values for a linear regression model. Each combination of unique parameter values forms a unique linear regression model, and the process of finding these optimal values is known as model fitting. Both approaches to model fitting we'll explore aim to minimize the following function:\n",
    "\n",
    "\n",
    "This function is the mean squared error between the predicted labels made using a given model and the true labels. The problem of choosing a set of values that minimize or maximize another function is known as an optimization problem [https://en.wikipedia.org/wiki/Mathematical_optimization].\n",
    "\n",
    "To build intuition for the optimization process, let's start with a single parameter linear regression model:\n",
    "\n",
    "y = a1x1\n",
    "\n",
    "Note that this is different from a simple linear regression model, which actually has two parameters: \n",
    "x0 and x1.\n",
    "\n",
    "Let's use the Gr Liv Area column for the single parameter:\n",
    "SalePrice = a1 * GrLivArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "numerical_train = train.select_dtypes(include=['int', 'float'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Correlating Feature Columns With Target Column\n",
    "\n",
    "In the last screen's widget, we observed how the optimization function follows a curve with a minimum value. This should remind you of our exploration of relative minimum values from calculus. If you recall, we computed the critical points by calculating the curve's derivative, setting it equal to \n",
    ", and finding the \n",
    " value at this point. Unfortunately, this approach won't work when we have multiple parameter values because minimizing one parameter value may increase another parameter's value. In addition, while we can plot the MSE curve when we only have a single parameter we're trying to find and visually select the value that minimizes the MSE, this approach won't work when we have multiple parameter value because we can't visualize past 3 dimensions.\n",
    "\n",
    "In this mission, we'll explore an iterative technique for solving this problem, known as gradient descent. The gradient descent algorithm [https://en.wikipedia.org/wiki/Gradient_descent] works by iteratively trying different parameter values until the model with the lowest mean squared error is found. Gradient descent is a commonly used optimization technique for other models as well, like neural networks, which we'll explore later in this track.\n",
    "\n",
    "Here's an overview of the gradient descent algorithm for a single parameter linear regression model:\n",
    "\n",
    "select initial values for the parameter: \n",
    "repeat until convergence (usually implemented with a max number of iterations):\n",
    "calculate the error (MSE) of model that uses current parameter value: \n",
    "calculate the derivative of the error (MSE) at the current parameter value: \n",
    "update the parameter value by subtracting the derivative times a constant (\n",
    ", called the learning rate): \n",
    "In the last step of the algorithm, you'll notice we used we used \n",
    " to indicate that the value on the right is assigned to the variable on the left. While in Python, we've used to the equals operator (=) for assignment, we've used it in math (\n",
    ") to signify equality. For example, a = 1 in Python assigns the value 1 to the variable a. In math, \n",
    " asserts that \n",
    " is equal to \n",
    ". In mathematical papers, sometimes \n",
    " is also used to signify assignment:\n",
    "\n",
    "\n",
    "Selecting an appropriate initial parameter and learning rate will reduce the number of iterations required to converge, and is part of hyperparameter optimization. We won't dive into those techniques in this course and will instead focus on how the algorithm works. In the next screen, we'll unpack how to calculate the derivative of the error function at each iteration of the algorithm."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Derivative Of The Cost Function\n",
    "\n",
    "In mathematical optimization, a function that we optimize through minimization is known as a cost function or sometime as the loss function [https://en.wikipedia.org/wiki/Loss_function]. Because we're trying to fit a single parameter model, we can replace with \n",
    " with \n",
    " in the cost function:\n",
    "\n",
    "\n",
    "In this screen, we'll apply calculus properties to simplify this derivative to something we can compute. We encourage you to follow along using pencil and paper, and see if you can apply the properties we mention at each step to obtain the same result we did. Note that while you'll probably never have to implement gradient descent yourself (as most packages have high performance implementations), understanding the math will help make it easier for you to debug when you run into issues.\n",
    "\n",
    "\n",
    "By applying the linearity of differentiation property [https://en.wikipedia.org/wiki/Linearity_of_differentiation] from calculus, we can bring the derivative term inside the summation:\n",
    "\n",
    "\n",
    "\n",
    "We can apply both the power rule and the chain rule to simplify this. You can read more about the chain rule here [https://en.wikipedia.org/wiki/Chain_rule] or observe how both are applied together here[https://www.khanacademy.org/math/calculus-home/taking-derivatives-calc/chain-rule-calc/v/differentiating-powers-of-functions]:\n",
    "\n",
    "\n",
    "Because we're differentiating \n",
    " with respect to \n",
    ", we treat \n",
    " and \n",
    " as constants. \n",
    " then simplifies to just \n",
    ":\n",
    "\n",
    "\n",
    "For every iteration of gradient descent:\n",
    "\n",
    "this derivative is computed using the current \n",
    "value\n",
    "the derivative is multiplied by the learning rate (\n",
    "): \n",
    "the result is subtracted from the current parameter value and assigned as the new parameter value: \n",
    "Here's what this would look like in code if we ran gradient descent for 10 iterations:\n",
    "\n",
    "a1_list = [1000]\n",
    "alpha = 10\n",
    "â€‹\n",
    "for x in range(0, 10):\n",
    "    a1 = a1_list[x]\n",
    "    deriv = derivative(a1, alpha, xi_list, yi_list)\n",
    "    a1_new = a1 - alpha*deriv\n",
    "    a1_list.append(a1_new)\n",
    "To test your understanding, implement the derivative() function.\n",
    "\n",
    "Instructions\n",
    "Finish implementing the derivative() function:\n",
    "This function should return the derivative at the current value of \n",
    ".\n",
    "Uncomment the 2 lines of code that run the gradient_descent() function, assign the list of iterations for the \n",
    " parameter to param_iterations, and assign the last iteration for \n",
    " to final_param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derivative(a1, xi_list, yi_list):\n",
    "    # Modify this function.\n",
    "    return 0\n",
    "\n",
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial):\n",
    "    a1_list = [a1_initial]\n",
    "\n",
    "    for i in range(0, max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        deriv = derivative(a1, xi_list, yi_list)\n",
    "        a1_new = a1 - alpha*deriv\n",
    "        a1_list.append(a1_new)\n",
    "    return(a1_list)\n",
    "\n",
    "# Uncomment when ready.\n",
    "# param_iterations = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150)\n",
    "# final_param = param_iterations[-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Understanding Multi Parameter Gradient Descent\n",
    "\n",
    "Now that we've understood how single parameter gradient descent works, let's build some intuition for multi parameter gradient descent. Let's start by visualizing the MSE as a function of the parameter values for the following simple linear regression model:\n",
    "\n",
    "\n",
    "In this screen's widget, we've generated a 3D scatter plot with:\n",
    "\n",
    "\n",
    " on the x-axis\n",
    "\n",
    " on the y-axis\n",
    "\n",
    " on the z-axis\n",
    "\n",
    "Instructions\n",
    "Filter the test data frame so it only contains the columns from final_corr_cols.index. Then, drop the row containing missing values and assign the result to clean_test\n",
    "Build a linear regression model using the features in features.\n",
    "Calculate the RMSE on the test and train sets.\n",
    "Assign the train RMSE to train_rmse and the test RMSE to test_rmse."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. Gradient Of The Cost Function\n",
    "\n",
    "In the widget from the first screen, you were able to use the parameter sliders to try to reduce the residual sum of squares (which, by proxy, also reduces the mean squared error). The gradient is a multi variable generalization of the derivative. In the last few screens, we were concerned with minimizing the following cost function:\n",
    "\n",
    "\n",
    "When we have 2 parameter values (\n",
    " and \n",
    "), the cost function is now a function of 2 variables, not 1:\n",
    "\n",
    "\n",
    "Instead of one update rule, we now need two update rules. We need one for \n",
    ":\n",
    "\n",
    "\n",
    "and one for \n",
    ":\n",
    "\n",
    "\n",
    "Earlier in this mission, we determined that \n",
    " worked out to \n",
    ". For the multiparameter case, we need to include the additional parameter :\n",
    "\n",
    "\n",
    "For \n",
    ", we won't walk through the proof for this derivative, but it's similar to the one we did for \n",
    " and we encourage you to derive this yourself on pencil and paper:\n",
    "\n",
    "\n",
    "Instructions\n",
    "Implement the a0_derivative() function, which implements the gradient for \n",
    ".\n",
    "Even though we're working on the multiparameter case, let's keep this function name consistent with the previous one we implemented (a1_derivative()).\n",
    "You'll notice that we added the a0 parameter to the function parameters. This is because we need both parameters for the individual parameter updates (verify this by looking at the math we explored in this screen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def a1_derivative(a0, a1, xi_list, yi_list):\n",
    "    len_data = len(xi_list)\n",
    "    error = 0\n",
    "    for i in range(0, len_data):\n",
    "        error += xi_list[i]*(a0 + a1*xi_list[i] - yi_list[i])\n",
    "    deriv = 2*error/len_data\n",
    "    return deriv\n",
    "\n",
    "def a0_derivative(a0, a1, xi_list, yi_list):\n",
    "    return 1\n",
    "\n",
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial, a0_initial):\n",
    "    a1_list = [a1_initial]\n",
    "    a0_list = [a0_initial]\n",
    "\n",
    "    for i in range(0, max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        a0 = a0_list[i]\n",
    "        \n",
    "        a1_deriv = a1_derivative(a0, a1, xi_list, yi_list)\n",
    "        a0_deriv = a0_derivative(a0, a1, xi_list, yi_list)\n",
    "        \n",
    "        a1_new = a1 - alpha*a1_deriv\n",
    "        a0_new = a0 - alpha*a0_deriv\n",
    "        \n",
    "        a1_list.append(a1_new)\n",
    "        a0_list.append(a0_new)\n",
    "    return(a0_list, a1_list)\n",
    "\n",
    "# Uncomment when ready.\n",
    "# a0_params, a1_params = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150, 1000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. Gradient Descent For Higher Dimensions\n",
    "\n",
    "What if we want to use many parameters in our model? Gradient descent actually scales to as many variables as you want. Each parameter value will need its own update rule, and it closely matches the update rule for \n",
    ":\n",
    "\n",
    "\n",
    "Besides the derivative for the MSE with respect to the intercept value (\n",
    "), the derivative for other parameters are identical:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
