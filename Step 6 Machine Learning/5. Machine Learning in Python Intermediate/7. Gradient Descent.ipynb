{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Machine Learning -  Gradient Descend"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Introduction to the data\n",
    "\n",
    "We have a dataset pga.csv containing professional golfers' driving statistics in two columns, accuracy and distance. Accuracy is measured as the percentage of fairways hit over many drives. Distances is measured as the average drive distance, in yards. Our goal is to predict accuracy using distance. In golf, it's expected that the further someone hits the ball the less accurate they will be. Lets see if this holds up.\n",
    "\n",
    "For many machine learning algorithms it's important to scale, or normalize, the data before using it. Here we have distance, measured in yards, and accuracy, measured in percentages. These two fields are on very different scales which can produce bias into learning algorithms. Many algorithms compute the Eucilidean Distance between two observations and if one of the features is vastly larger than another, the distance will be biased towards that particular feature. To normalize the data, for each value, subtract each the mean and then divide by the standard deviation.\n",
    "\n",
    "After normalizing the data, we plot the data to get a visual sense of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read data from csv\n",
    "pga = pandas.read_csv(\"pga.csv\")\n",
    "\n",
    "# Normalize the data\n",
    "pga.distance = (pga.distance - pga.distance.mean()) / pga.distance.std()\n",
    "pga.accuracy = (pga.accuracy - pga.accuracy.mean()) / pga.accuracy.std()\n",
    "print(pga.head())\n",
    "\n",
    "plt.scatter(pga.distance, pga.accuracy)\n",
    "plt.xlabel('normalized distance')\n",
    "plt.ylabel('normalized accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Linear model\n",
    "\n",
    "From this plot, the data looks linear with a negative slope, lower accuracy with higher distance. We can use a linear model, as shown in previous missions, to model this data. This model is written as \n",
    " where \n",
    "'s are coefficients and \n",
    " are error terms. To start, lets use sklearn's LinearRegression class to estimate a linear model.\n",
    "\n",
    "Instructions\n",
    "Fit a linear model where distance is the independent variable and accuracy is the dependent variable.\n",
    "Use the sklearn class LinearRegression and assign the coefficient of distance to theta1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# We can add a dimension to an array by using np.newaxis\n",
    "print(\"Shape of the series:\", pga.distance.shape)\n",
    "print(\"Shape with newaxis:\", pga.distance[:, np.newaxis].shape)\n",
    "\n",
    "# The X variable in LinearRegression.fit() must have 2 dimensions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Cost function, introduction\n",
    "\n",
    "We utilized a pre-existing library sklearn to estimate the coefficients of our linear model, using least squares. The least squares method can effectively fit linear models since it only requires matrix algebra and provides deterministic estimates of the coefficients. Least squares is a method which directly minimized the sum of square error in a model algebraically. Often times we have too much data to fit into memory and we can't use least squares.\n",
    "\n",
    "Gradient descent is a general method that can be used to estimate coefficents of nearly any model, including linear models. At it's core, gradient descent minimizes the residuals in the estimated model by updating each coefficent based on it's gradient.\n",
    "\n",
    "To start we must understand cost functions. Most cost functions measure the difference between a model predictions and it's corresponding observations with the coefficients as parameters. Lets say our model is \n",
    ".\n",
    "\n",
    "The cost function is then defined as, \n",
    ". The cost here is one half the average difference between our prediction and observation squared. As we change the coefficients of the model this cost changes. During modeling we will randomly choose the coefficients and update them intelligently to minimize this cost.\n",
    "\n",
    "Instructions\n",
    "Compute the cost for each theta1 in theta1s and theta0=100.\n",
    "Create a plot with theta1s on the x-axis and the corresponding costs on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The cost function of a single variable linear model\n",
    "def cost(theta0, theta1, x, y):\n",
    "    # Initialize cost\n",
    "    J = 0\n",
    "    # The number of observations\n",
    "    m = len(x)\n",
    "    # Loop through each observation\n",
    "    for i in range(m):\n",
    "        # Compute the hypothesis \n",
    "        h = theta1 * x[i] + theta0\n",
    "        # Add to cost\n",
    "        J += (h - y[i])**2\n",
    "    # Average and normalize cost\n",
    "    J /= (2*m)\n",
    "    return J\n",
    "\n",
    "# The cost for theta0=0 and theta1=1\n",
    "print(cost(0, 1, pga.distance, pga.accuracy))\n",
    "\n",
    "theta0 = 100\n",
    "theta1s = np.linspace(-3,2,100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Cost function, continued\n",
    "\n",
    "The cost function above is quadratic, like a parabola, with respect to the slope and we can see there is a global minimum. A global minimum is the point where the function has the lowest value. We need to find the best set of parameters to minimize the cost function, but here we are only varying the slope and keeping the intercept constant. The minimum of the cost function is the point where the model has the lowest error, hence the point where our parameters are optimized. Instead we can use a 3D plot to visualize this cost function where the x and y axis will be the slope and intercept and the z axis will be the cost.\n",
    "\n",
    "Instructions\n",
    "Make a 3D surface plot with theta0s on the x-axis, theta1s on the y-axis, and the corresponding cost on the z-axis.\n",
    "Use the cost function to calculate the cost.\n",
    "Assign each cost into the respective index in variable cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Example of a Surface Plot using Matplotlib\n",
    "# Create x an y variables\n",
    "x = np.linspace(-10,10,100)\n",
    "y = np.linspace(-10,10,100)\n",
    "\n",
    "# We must create variables to represent each possible pair of points in x and y\n",
    "# ie. (-10, 10), (-10, -9.8), ... (0, 0), ... ,(10, 9.8), (10,9.8)\n",
    "# x and y need to be transformed to 100x100 matrices to represent these coordinates\n",
    "# np.meshgrid will build a coordinate matrices of x and y\n",
    "X, Y = np.meshgrid(x,y)\n",
    "print(X[:5,:5],\"\\n\",Y[:5,:5])\n",
    "\n",
    "# Compute a 3D parabola \n",
    "Z = X**2 + Y**2 \n",
    "\n",
    "# Open a figure to place the plot on\n",
    "fig = plt.figure()\n",
    "# Initialize 3D plot\n",
    "ax = fig.gca(projection='3d')\n",
    "# Plot the surface\n",
    "ax.plot_surface(X=X,Y=Y,Z=Z)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Use these for your exercise\n",
    "theta0s = np.linspace(-2,2,100)\n",
    "theta1s = np.linspace(-2,2, 100)\n",
    "COST = np.empty(shape=(100,100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. Cost function, slopes\n",
    "\n",
    "Gradient descent relies on finding the direction of the largest gradient where a gradient is the \"slope\" of a multivariable function. To find this gradient we can take the partial derivative in terms of each parameter in the cost function. A partial derivative represents the slope of a multivariable function in terms of a single parameter, ie. holding all other variables constant, what is the slope in the direction of the one parameter. In the case of this cost function, we will take the partial derivatives in terms of \n",
    " and \n",
    ". Visually, looking at the 3D plot above, we want to find the slope of the function in the direction either the x or y axis. If you are not familiar with derivatives do not worry, we will not require you to derive any in this lesson. Most importantly we must remember that we are just finding the incline of a function relative to each parameter.\n",
    "\n",
    "\n",
    " is read as the partial derivative of \n",
    "in terms of \n",
    ". This is not part of the equation but just the representation of partial derivatives.\n",
    "\n",
    "The partial derivative of the cost function in terms of \n",
    " is:  \n",
    ".\n",
    "\n",
    "The partial deriviate of the cost function in terms of \n",
    " is: \n",
    "We've written the code to compute the partial derivative in terms of \n",
    " below. theta0 and theta1 are inputs to the function to give a reference point of where to take the derivative from. x is our feature vector and y are the observed, target, values. We then find the error between our observations and hypothesised model and multiply by x. The average of all these terms is then the partial derivative. This function gives us the slope in the direction of the \n",
    "coefficient.\n",
    "\n",
    "Instructions\n",
    "Write a function named partial_cost_theta0(theta0, theta1, x, y) to compute \n",
    ".\n",
    "theta0 and theta1 are initial parameters of the linear model, x is our feature vector (distance) and y are the observations (accuracy).\n",
    "Assign the partial derivative where theta0=1, theta1=1, x=pga.distance, and y=pga.accuracy to variable partial0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_cost_theta1(theta0, theta1, x, y):\n",
    "    # Hypothesis\n",
    "    h = theta0 + theta1*x\n",
    "    # Hypothesis minus observed times x\n",
    "    diff = (h - y) * x\n",
    "    # Average to compute partial derivative\n",
    "    partial = diff.sum() / (x.shape[0])\n",
    "    return partial\n",
    "\n",
    "partial1 = partial_cost_theta1(0, 5, pga.distance, pga.accuracy)\n",
    "print(\"partial1 =\", partial1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. Gradient descent algorithm\n",
    "\n",
    "Visually, we see that by varying our slope and intercept we get drastically different costs. In order to minimize the error between our hypothesised model and observations we can find the minimum of the cost function by changing the parameters. Gradient descent is a widely used method to find the optimal parameters. To execute gradient descent we randomly initialize a set of parameters and update them by moving in the direction of the cost function's steepest slope, ie. the descending down the function. If we can find the downward slope in terms of each parameter we can move in the direction of the global minumum. Eventually the updates will converge to a near optimal set of parameters. When parameters converge the hypothesised parameters become very close to the optimal parameters. We measure convergence by finding the difference between the previous iterations cost versus the current cost.\n",
    "\n",
    "The general gradient descent algorithm for two variables is:\n",
    "\n",
    "repeat until convergence {        \n",
    "        \n",
    " }\n",
    "\n",
    "Let's go through this term by term.  \n",
    " is the current value of our coefficient, ie. how much accuracy is lost per yard of distance.  \n",
    " is the learning rate. This value is set by the user and controls how fast the algorithm will converge by changing the parameters by some percentage of the slope. Values of this learning rate can vary from project to project but in general learning rates can be between 0.0001 and 1. This value must not be too large or the algorithm will overshoot the minimum but if it's too small it will take many iterations to converge.  \n",
    " is the partial derivative of our cost function in terms of \n",
    " and \n",
    " is the partial derivative of our cost function in terms of \n",
    ". These measure the partial derivatives in relation to our coefficients. Since we want to minimize the cost function we substract the partial derivatives times some learning rate from our coefficients to get our new set of coefficients.\n",
    "\n",
    "We will start by initializing a few variables. updates will store our convergence data for visualization later. theta0 and theta1 will hold initial values of the slope and intercept. alpha is used for our learning rate. Finding a learning rate is often done by trial and error. A good starting point is 0.01. If you find that the algorithm is learning too slowly it can be increased. If the cost starts increasing out of control then the learning rate is probably overshooting the minimum and should be decreased. We will then use the max_epochs to limit the number of iterations so it doesn't run forever. c will be used to hold the initial cost using the initial parameters.\n",
    "\n",
    "Instructions\n",
    "Execute the gradient descent algorithm with alpha=0.01, x=pga.distance, and y=pga.accuracy.\n",
    "Make a plot of costs on the y-axis and the iteration (0 to len(costs)) on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x is our feature vector -- distance\n",
    "# y is our target variable -- accuracy\n",
    "# alpha is the learning rate\n",
    "# theta0 is the intial theta0 \n",
    "# theta1 is the intial theta1\n",
    "def gradient_descent(x, y, alpha=0.1, theta0=0, theta1=0):\n",
    "    max_epochs = 1000 # Maximum number of iterations\n",
    "    counter = 0      # Intialize a counter\n",
    "    c = cost(theta1, theta0, pga.distance, pga.accuracy)  ## Initial cost\n",
    "    costs = [c]     # Lets store each update\n",
    "    # Set a convergence threshold to find where the cost function in minimized\n",
    "    # When the difference between the previous cost and current cost \n",
    "    #        is less than this value we will say the parameters converged\n",
    "    convergence_thres = 0.000001  \n",
    "    cprev = c + 10   \n",
    "    theta0s = [theta0]\n",
    "    theta1s = [theta1]\n",
    "\n",
    "    # When the costs converge or we hit a large number of iterations will we stop updating\n",
    "    while (np.abs(cprev - c) > convergence_thres) and (counter < max_epochs):\n",
    "        cprev = c\n",
    "        # Alpha times the partial deriviative is our updated\n",
    "        update0 = alpha * partial_cost_theta0(theta0, theta1, x, y)\n",
    "        update1 = alpha * partial_cost_theta1(theta0, theta1, x, y)\n",
    "\n",
    "        # Update theta0 and theta1 at the same time\n",
    "        # We want to compute the slopes at the same set of hypothesised parameters\n",
    "        #             so we update after finding the partial derivatives\n",
    "        theta0 -= update0\n",
    "        theta1 -= update1\n",
    "        \n",
    "        # Store thetas\n",
    "        theta0s.append(theta0)\n",
    "        theta1s.append(theta1)\n",
    "        \n",
    "        # Compute the new cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
