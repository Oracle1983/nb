{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Machine Learning - Hyperameter Optimization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Recap\n",
    "\n",
    "In the last mission, we focused on increasing the number of attributes the model uses. We saw how, in general, adding more attributes generally lowered the error of the model. This is because the model is able to do a better job identifying the living spaces from the training set that are the most similar to the ones from the test set. However, we also observed how using all of the available features didn't actually improve the model's accuracy automatically and that some of the features were probably not relevant for similarity ranking. We learned that selecting relevant features was the right lever when improving a model's accuracy, not just increasing the features used in the absolute.\n",
    "\n",
    "In this mission, we'll focus on the impact of increasing k, the number of nearby neighbors the model uses to make predictions. We exported both the training (train_df) and test sets (test_df) from the last missions to CSV files, dc_airbnb_train.csv and dc_airbnb_test.csv respectively. Let's read both these CSV's into Dataframes.\n",
    "\n",
    "Instructions\n",
    "Read dc_airbnb_train.csv into a Dataframe and assign to train_df.\n",
    "Read dc_airbnb_test.csv into a Dataframe and assign to test_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Hyperparameter optimization\n",
    "\n",
    "When we vary the features that are used in the model, we're affecting the data that the model uses. On the other hand, varying the k value affects the behavior of the model independently of the actual data that's used when making predictions. In other words, we're impacting how the model performs without trying to change the data that's used.\n",
    "\n",
    "Values that affect the behavior and performance of a model that are unrelated to the data that's used are referred to as hyperparameters. The process of finding the optimal hyperparameter value is known as hyperparameter optimization. A simple but common hyperparameter optimization technique [https://en.wikipedia.org/wiki/Hyperparameter_optimization] is known as grid search [https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search], which involves:\n",
    "\n",
    "selecting a subset of the possible hyperparameter values,\n",
    "training a model using each of these hyperparameter values,\n",
    "evaluating each model's performance,\n",
    "selecting the hyperparameter value that resulted in the lowest error value.\n",
    "Grid search essentially boils down to evaluating the model performance at different k values and selecting the k value that resulted in the lowest error. While grid search can take a long time when working with large datasets, the data we're working with in this mission is small and this process is relatively quick.\n",
    "\n",
    "Let's confirm that grid search will work quickly for the dataset we're working with by first observing how the model performance changes as we increase the k value from 1 to 5. If you recall, we set 5 as the k value for the last 2 missions. Let's use the features from the last mission that resulted in the best model accuracy:\n",
    "\n",
    "accommodates\n",
    "bedrooms\n",
    "bathrooms\n",
    "number_of_reviews\n",
    "\n",
    "Instructions\n",
    "Create a list containing the integer values 1, 2, 3, 4, and 5, in that order, and assign to hyper_params.\n",
    "Create an empty list and assign to mse_values.\n",
    "Use a for loop to iterate over hyper_params and in each iteration:\n",
    "Instantiate a KNeighborsRegressor object with the following parameters:\n",
    "n_neighbors: the current value for the iterator variable,\n",
    "algorithm: brute\n",
    "Fit the instantiated k-nearest neighbors model to the following columns from train_df:\n",
    "accommodates\n",
    "bedrooms\n",
    "bathrooms\n",
    "number_of_reviews\n",
    "Use the trained model to make predictions on the same columns from test_df and assign to predictions.\n",
    "Use the mean_squared_error function to calculate the MSE value between predictions and the price column from test_df.\n",
    "Append the MSE value to mse_values.\n",
    "Display mse_values using the print() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Expanding grid search\n",
    "\n",
    "Since our dataset is small and scikit-learn has been developed with performance in mind, the code ran quickly. As we increased the k value from 1 to 5, the MSE value fell from approximately 26364 to approximately 14090:\n",
    "\n",
    "k\tMSE\n",
    "1\t26364.928327645051\n",
    "2\t15100.522468714449\n",
    "3\t14579.597901655923\n",
    "4\t16212.300767918088\n",
    "5\t14090.011649601822\n",
    "Let's expand grid search all the way to a k value of 20. While 20 may seem like an arbitrary ending point for our grid search, we can always expand the values we try if we're unconvinced that the lowest MSE value is associated with one of the hyperparamter values we tried so far.\n",
    "\n",
    "Instructions\n",
    "Change the list of hyperparameter values, hyper_params, so it ranges from 1 to 20.\n",
    "Create an empty list and assign to mse_values.\n",
    "Use a for loop to iterate over hyper_params and in each iteration:\n",
    "Instantiate a KNeighborsRegressor object with the following parameters:\n",
    "n_neighbors: the current value for the iterator variable,\n",
    "algorithm: brute\n",
    "Fit the instantiated k-nearest neighbors model to the following columns from train_df:\n",
    "accommodates\n",
    "bedrooms\n",
    "bathrooms\n",
    "number_of_reviews\n",
    "Use the trained model to make predictions on the same columns from test_df and assign to predictions.\n",
    "Use the mean_squared_error function to calculate the MSE value between predictions and the price column from test_df.\n",
    "Append the MSE value to mse_values.\n",
    "Display mse_values using the print() function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Visualizing hyperparameter values\n",
    "\n",
    "As we increased the k value from 1 to 6, the MSE value decreased from approximately 26364 to approximately 13657. However, as we increased the k value from 7 to 20, the MSE value didn't decrease further but instead hovered between approximately 14288 and 14870. This means that the optimal k value is 6, since it resulted in the lowest MSE value.\n",
    "\n",
    "This pattern is something you'll notice while performing grid search across other models as well. As you increase k at first, the error rate decreases until a certain point, but then rebounds and increases again. Let's confirm this behavior visually using a scatter plot\n",
    "\n",
    "Instructions\n",
    "Use the scatter() method from matplotlib.pyplot to generate a line plot with:\n",
    "hyper_params on the x-axis,\n",
    "mse_values on the y-axis.\n",
    "Use plt.show() to display the line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "features = ['accommodates', 'bedrooms', 'bathrooms', 'number_of_reviews']\n",
    "hyper_params = [x for x in range(1, 21)]\n",
    "mse_values = list()\n",
    "\n",
    "for hp in hyper_params:\n",
    "    knn = KNeighborsRegressor(n_neighbors=hp, algorithm='brute')\n",
    "    knn.fit(train_df[features], train_df['price'])\n",
    "    predictions = knn.predict(test_df[features])\n",
    "    mse = mean_squared_error(test_df['price'], predictions)\n",
    "    mse_values.append(mse)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. Varying features and hyperparameters\n",
    "\n",
    "From the scatter plot, you can tell that the lowest MSE value was achieved at the k value of 6. As we increased k past 6, the MSE actually increased and hovered but never decreased below 13657 (the approximate MSE value when k was 6).\n",
    "\n",
    "Since varying the k value decreased the MSE value for this model, you may be wondering if repeating the grid search process for one of the models from the last mission that performed poorly when we fixed k to 5 would result in a lower MSE value. Let's try it out!\n",
    "\n",
    "Instructions\n",
    "Use a for loop to iterate over hyper_params and in each iteration:\n",
    "Instantiate a KNeighborsRegressor object with the following parameters:\n",
    "n_neighbors: the current value for the iterator variable,\n",
    "algorithm: brute\n",
    "Fit the instantiated k-nearest neighbors model to all of the columns, except for the price column, from train_df\n",
    "Use the trained model to make predictions on the same columns from test_df and assign to predictions.\n",
    "Use the mean_squared_error function to calculate the MSE value between predictions and the price column from test_df.\n",
    "Append the MSE value to mse_values.\n",
    "Use the scatter() method from matplotlib.pyplot to generate a line plot with:\n",
    "hyper_params on the x-axis,\n",
    "mse_values on the y-axis.\n",
    "Use plt.show() to display the line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyper_params = [x for x in range(1,21)]\n",
    "mse_values = list()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. Practice the workflow\n",
    "\n",
    "You may have noticed that the general workflow for finding the best model is:\n",
    "\n",
    "select relevant features to use for predicting the target column.\n",
    "use grid search to find the optimal hyperparameter value for the selected features.\n",
    "evaluate the model's accuracy and repeat the process.\n",
    "Let's now practice this workflow.\n",
    "\n",
    "Instructions\n",
    "While using only the accommodates and bathrooms columns:\n",
    "Train a model for each k value between 1 and 20 using the training data.\n",
    "Use each model to make predictions on the test set (using just the accommodates and bathrooms columns).\n",
    "Calculate each model's MSE value by comparing each set of predictions to the true price values.\n",
    "Find the k value that obtained the lowest MSE value.\n",
    "Create a dictionary named two_hyp_mse that contains 1 key-value pair:\n",
    "key: k value that resulted in lowest MSE value.\n",
    "value: corresponding MSE value.\n",
    "Repeat this process while using only the accommodates, bathrooms, and bedrooms columns:\n",
    "Create a dictionary named three_hyp_mse that contains 1 key-value pair:\n",
    "key: k value that resulted in lowest MSE value.\n",
    "value: corresponding MSE value.\n",
    "Display both two_hyp_mse and three_hyp_mse using the print() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "two_features = ['accommodates', 'bathrooms']\n",
    "three_features = ['accommodates', 'bathrooms', 'bedrooms']\n",
    "hyper_params = [x for x in range(1,21)]\n",
    "# Append the first model's MSE values to this list.\n",
    "two_mse_values = list()\n",
    "# Append the second model's MSE values to this list.\n",
    "three_mse_values = list()\n",
    "two_hyp_mse = dict()\n",
    "three_hyp_mse = dict()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "7. Next Steps\n",
    "\n",
    "The first model, which used the accommodates and bathrooms columns, was able to achieve an MSE value of approximately 14790. The second model, which added the bedrooms column, was able to achieve an MSE value of approximately 13522.9, which is even lower than the lowest MSE value we achieved using the best model from the last mission (which used the accommodates, bedrooms, bathrooms, and number_of_reviews columns). Hopefully this demonstrates that using just one lever to find the best model isn't enough and you really want to use both levers in conjunction.\n",
    "\n",
    "In this mission, we learned about hyperparameter optimization and the workflow of finding the optimal model to make predictions. Next in this course is a challenge, where you'll practice the concepts you've learned so far on a completely new dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
